{"pages":[],"posts":[{"title":"dubbo学习","text":"#学习资料 ​ 学习的最好资料来源是官方文档和源码 ​ 这里官方已经很全面了，包括了SPI，服务的引入导出，路由，集群，均衡策略，服务调用过程都做了详细的源码分析。 详细参见：官方Dubbo 服务调用过程 #MOCK测试平台思路： ​ 参考dubbo的mock方式，实际项目中测试需要一些数据及结果流程比较长而且复杂，可以抽象一种类似的MOCK方式来简化此过程，参见基于 Dubbo 分布式服务框架的一种 MOCK 方式 使用mockito框架mock掉整个RedisTemplate的示例 回声测试 回声测试用于检测服务是否可用，回声测试按照正常请求流程执行，能够测试整个调用是否畅通，可用于监控。 所有服务自动实现EchoService接口，只需要将任意服务引用强制转换为EchoService，即可使用。 Dubbo中的回声测试是如何实现的？为什么能将任意的服务引用都强制转为EchoService？ 流程： java中关于强制转换的一个限制：必须有继承关系，就是说两个类之间要能够进行类型转换，必须有继承关系才可以。 可是很明显，我们写的Dubbo服务接口是与EchoService接口没有任何集成关系的，这是如何实现的呢？ 服务创建动态代理的时候，是在传入的接口中人为的增加了“EchoService.class”接口 EchoService接口中的方法是怎么实现的呢? 因为Dubbo提供了很多Filter，针对这个EchoService提供了一个EchoFilter实现 12EchoService echoService = (EchoService) demoService;System.out.println(echoService.$echo(\"hello\")); 隐式参数&amp;上下文dubbo一些你不一定知道但是很好用的功能","link":"/2019/12/05/dubbo-pre/"},{"title":"lru算法","text":"简介​ LRU（Least Recently Used）直译为“最近最少使用”。其实很多老外发明的词直译过来对于我们来说并不是特别好理解，甚至有些词并不在国人的思维模式之内，比如快速排序中的Pivot，模拟信号中的Analog 等等。笔者认为最好的理解方式就是看他诞生的原因，看这个概念的出现如何一步一步演变为现在的样子。为了力求方便理解，下面我们先来看看LRU是什么，主要是为了解决什么问题。 ​ 我们的内存是有限的。所以当缓存数据在内存越来越多，以至于无法存放即将到来的新缓存数据时，就必须扔掉最不常用的缓存数据。所以对于LRU的抽象总结如下： 缓存的容量是有限的 当缓存容量不足以存放需要缓存的新数据时，必须丢掉最不常用的缓存数据 实现LRU​ 1.用一个数组来存储数据，给每一个数据项标记一个访问时间戳，每次插入新数据项的时候，先把数组中存在的数据项的时间戳更新。当数组空间已满时，将时间戳最小的数据项淘汰。 ​ 2.利用一个链表来实现，每次新插入数据的时候将新数据插到链表的头部；每次缓存命中（即数据被访问），则将数据移到链表头部；那么当链表满的时候，就将链表尾部的数据丢弃。 ​ 3.利用链表和hashmap。当需要插入新的数据项的时候，如果新数据项在链表中存在（一般称为命中），则把该节点移到链表头部，如果不存在，则新建一个节点，放到链表头部，若缓存满了，则把链表最后一个节点删除即可。在访问数据的时候，如果数据项在链表中存在，则把该节点移到链表头部，否则返回-1。这样一来在链表尾部的节点就是最近最久未访问的数据项。 比较 ​ 对于第一种方法，需要不停地维护数据项的访问时间戳，另外，在插入数据、删除数据以及访问数据时，时间复杂度都是O(n)。对于第二种方法，链表在定位数据的时候时间复杂度为O(n)。所以在一般使用第三种方式来是实现LRU算法。 1234567891011121314151617181920212223public class LruLinkedHashMap&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; { private Integer cacheSize; public LruLinkedHashMap(int initialCapacity) { // 由于LinkedHashMap是为自动扩容的，当table数组中元素大于Capacity * loadFactor的时候，就会自动进行两倍扩容。但是为了使缓存大小固定，就需要在初始化的时候传入容量大小和负载因子。为了使得到达设置缓存大小不会进行自动扩容，需要将初始化的大小进行计算再传入 int capacity = (int)Math.ceil(initialCapacity / 0.75f) + 1; /* * 第三个参数设置为true，代表linkedlist按访问顺序排序，可作为LRU缓存 * 第三个参数设置为false，代表按插入顺序排序，可作为FIFO缓存 */ super(capacity, 0.75f, true); cacheSize = initialCapacity; } @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { //当返回true的时候，就会remove其中最久的元素 return size() &gt; cacheSize; }} 参见：LRU","link":"/2019/11/28/algorithm-lru/"},{"title":"epoll理论解读","text":"I/O的操作 首先我们来定义流的概念，一个流可以是文件，socket，pipe等等可以进行I/O操作的内核对象。 不管是文件，还是套接字，还是管道，我们都可以把他们看作流。 之后我们来讨论I/O的操作，通过read，我们可以从流中读入数据；通过write，我们可以往流写入数据。现在假定一个情形，我们需要从流中读数据，但是流中还没有数据，（典型的例子为，客户端要从socket读如数据，但是服务器还没有把数据传回来），这时候该怎么办？ 阻塞非阻塞阻塞I/O模式阻塞I/O模式下，一个线程只能处理一个流的I/O事件。如果想要同时处理多个流，要么多进程(fork)，要么多线程(pthread_create)，很不幸这两种方法效率都不高。 非阻塞忙轮询的I/O 于是再来考虑非阻塞忙轮询的I/O方式，我们发现我们可以同时处理多个流了（把一个流从阻塞模式切换到非阻塞模式再此不予讨论）： while true { for i in stream[]; { if i has data read until unavailable } } 我们只要不停的把所有流从头到尾问一遍，又从头开始。这样就可以处理多个流了，但这样的做法显然不好，因为如果所有的流都没有数据，那么只会白白浪费CPU。这里要补充一点，阻塞模式下，内核对于I/O事件的处理是阻塞或者唤醒，而非阻塞模式下则把I/O事件交给其他对象（后文介绍的select以及epoll）处理甚至直接忽略。 select 为了避免CPU空转，可以引进了一个代理（一开始有一位叫做select的代理，后来又有一位叫做poll的代理，不过两者的本质是一样的）。这个代理比较厉害，可以同时观察许多流的I/O事件，在空闲的时候，会把当前线程阻塞掉，当有一个或多个流有I/O事件时，就从阻塞态中醒来，于是我们的程序就会轮询一遍所有的流（于是我们可以把“忙”字去掉了）。代码长这样: while true { select(streams[]) for i in streams[] { if i has data read until unavailable } } 于是，如果没有I/O事件产生，我们的程序就会阻塞在select处。但是依然有个问题，我们从select那里仅仅知道了，有I/O事件发生了，但却并不知道是那几个流（可能有一个，多个，甚至全部），我们只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。 但是使用select，我们有O(n)的无差别轮询复杂度，同时处理的流越多，没一次无差别轮询时间就越长。再次 说了这么多，终于能好好解释epoll了 epoll epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll之会把哪个流发生了怎样的I/O事件通知我们。此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） 在讨论epoll的实现细节之前，先把epoll的相关操作列出： epoll_create 创建一个epoll对象，一般epollfd = epoll_create() epoll_ctl （epoll_add/epoll_del的合体），往epoll对象中增加/删除某一个流的某一个事件 比如 epoll_ctl(epollfd, EPOLL_CTL_ADD, socket, EPOLLIN);//注册缓冲区非空事件，即有数据流入 epoll_ctl(epollfd, EPOLL_CTL_DEL, socket, EPOLLOUT);//注册缓冲区非满事件，即流可以被写入 epoll_wait(epollfd,…)等待直到注册的事件发生 （注：当对一个非阻塞流的读写发生缓冲区满或缓冲区空，write/read会返回-1，并设置errno=EAGAIN。而epoll只关心缓冲区非满和缓冲区非空事件）。 一个epoll模式的代码大概的样子是： while true { active_stream[] = epoll_wait(epollfd) for i in active_stream[] { read or write till } } 参照： 通俗解读：https://blog.csdn.net/u011671986/article/details/79449853 详细解读：https://bbs.gameres.com/thread_842984_1_1.html","link":"/2019/11/25/epoll/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/11/14/hello-world/"},{"title":"http1.x、http 2和https","text":"一、HTTP/1.xHttp1.x 缺陷：线程阻塞，在同一时间，同一域名的请求有一定数量限制，超过限制数目的请求会被阻塞 http1.0 缺陷：浏览器与服务器只保持短暂的连接，浏览器的每次请求都需要与服务器建立一个TCP连接（TCP连接的新建成本很高，因为需要客户端和服务器三次握手），服务器完成请求处理后立即断开TCP连接，服务器不跟踪每个客户也不记录过去的请求； 解决方案： 添加头信息——非标准的Connection字段Connection: keep-alive http1.1： 改进点： 持久连接 引入了持久连接，即TCP连接默认不关闭，可以被多个请求复用，不用声明Connection: keep-alive(对于同一个域名，大多数浏览器允许同时建立6个持久连接) 管道机制 即在同一个TCP连接里面，客户端可以同时发送多个请求。 分块传输编码 即服务端没产生一块数据，就发送一块，采用”流模式”而取代”缓存模式”。 新增请求方式 PUT:请求服务器存储一个资源; DELETE：请求服务器删除标识的资源； OPTIONS：请求查询服务器的性能，或者查询与资源相关的选项和需求； TRACE：请求服务器回送收到的请求信息，主要用于测试或诊断； CONNECT：保留将来使用 缺点： 虽然允许复用TCP连接，但是同一个TCP连接里面，所有的数据通信是按次序进行的。服务器只有处理完一个请求，才会接着处理下一个请求。如果前面的处理特别慢，后面就会有许多请求排队等着。这将导致“队头堵塞” 避免方式：一是减少请求数，二是同时多开持久连接 二、HTTP/2.0 特点： 采用二进制格式而非文本格式； 完全多路复用，而非有序并阻塞的、只需一个连接即可实现并行； 使用报头压缩，降低开销 服务器推送 1. 二进制协议 HTTP/1.1 版的头信息肯定是文本（ASCII编码），数据体可以是文本，也可以是二进制。HTTP/2 则是一个彻底的二进制协议，头信息和数据体都是二进制，并且统称为”帧”：头信息帧和数据帧。 二进制协议解析起来更高效、“线上”更紧凑，更重要的是错误更少。 2. 完全多路复用 HTTP/2 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了”队头堵塞”。 3. 报头压缩 HTTP 协议是没有状态，导致每次请求都必须附上所有信息。所以，请求的很多头字段都是重复的，比如Cookie，一样的内容每次请求都必须附带，这会浪费很多带宽，也影响速度。 对于相同的头部，不必再通过请求发送，只需发送一次； HTTP/2 对这一点做了优化，引入了头信息压缩机制； 一方面，头信息使用gzip或compress压缩后再发送； 另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，产生一个索引号，之后就不发送同样字段了，只需发送索引号。 4. 服务器推送 HTTP/2 允许服务器未经请求，主动向客户端发送资源； 通过推送那些服务器任务客户端将会需要的内容到客户端的缓存中，避免往返的延迟 三、HTTPS HTTP协议通常承载于TCP协议之上，在HTTP和TCP之间添加一个安全协议层（SSL或TSL），这个时候，就成了我们常说的HTTPS. 1、HTTPS主要作用12- （1）对数据进行加密，并建立一个信息安全通道，来保证传输过程中的数据安全;- （2）对网站服务器进行真实身份认证。 2、HTTPS和HTTP的区别12345- 1、HTTPS是加密传输协议，HTTP是名文传输协议;- 2、HTTPS需要用到SSL证书，而HTTP不用;- 3、HTTPS比HTTP更加安全，对搜索引擎更友好，利于SEO,- 4、HTTPS标准端口443，HTTP标准端口80;- 5、HTTPS基于传输层，HTTP基于应用层; 3、HTTPS和HTTP的工作过程区别 HTTP 包含动作： 浏览器打开一个 TCP 连接 浏览器发送 HTTP 请求到服务器端 服务器发送 HTTP 回应信息到浏览器 TCP 连接关闭 SSL 包含动作： 验证服务器端 客户端和服务器端选择加密算法和密码，确保双方都支持 验证客户端(可选) 使用公钥加密技术来生成共享加密数据 创建一个加密的 SSL 连接 基于该 SSL 连接传递 HTTP 请求 4、HTTPS加密方式 对称加密：加密和解密都是使用的同一个密钥； 非对称加密： 加密使用的密钥和解密使用的密钥是不相同的，分别称为：公钥、私钥； 公钥和算法都是公开的，私钥是保密的。 非对称加密过程： 服务端生成配对的公钥和私钥 私钥保存在服务端，公钥发送给客户端 客户端使用公钥加密明文传输给服务端 服务端使用私钥解密密文得到明文 数字签名：签名就是在信息的后面再加上一段内容，可以证明信息没有被修改过。 引用：深入理解http1.x、http 2和https HTTP/2详解 HTTP与HTTPS的区别","link":"/2019/12/13/http-pre/"},{"title":"I/O模型(同步，异步，阻塞，非阻塞)","text":"思考：​ 同步（synchronous） IO和异步（asynchronous） IO，阻塞（blocking） IO和非阻塞（non-blocking）IO分别是什么，到底有什么区别？ ​ 用户进程，系统进程注意区分 IO发生时涉及的对象和步骤​ 对于一个network IO (这里我们以read举例)，它会涉及到两个系统对象，一个是调用这个IO的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the proces) 记住这两点很重要，因为这些IO Model的区别就是在两个阶段上各有不同的情况。 Stevens在文章中一共比较了五种IO Model blocking IO(阻塞) nonblocking IO(非阻塞) IO multiplexing（多路复用） signal driven IO（信号驱动） asynchronous IO（异步） blocking IO ​ 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。对于network io来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候kernel就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 non-blocking IO ​ 当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。所以，用户进程其实是需要不断的主动询问kernel数据好了没有。 IO multiplexing​ IO multiplexing这个词可能有点陌生，但是如果我说select，epoll，大概就都能明白了。有些地方也称这种IO方式为event driven IO。我们都知道，select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。它的流程如图： ​ 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。 这个图和阻塞IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）​ 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的进程其实是一直被block的。只不过进程是被select这个函数block，而不是被socket IO给block。​ signal driven IO Asynchronous I/O​ 这类函数的工作机制是告知内核启动某个操作，并让内核在整个操作（包括将数据从内核拷贝到用户空间）完成后通知我们。 ​ 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 QAblocking和non-blocking的区别​ 调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 synchronous IO和asynchronous IO的区别​ Stevens给出的定义（其实是POSIX的定义）是这样子的：​ A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;​ An asynchronous I/O operation does not cause the requesting process to be blocked; ​ 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。 ​ 定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。 non-blocking IO和asynchronous IO的区别​ 区别在于非阻塞在第一个阶段check第二阶段block，异步全程交给kenel处理 参照https://blog.csdn.net/historyasamirror/article/details/5778378 Richard Stevens的“UNIX® Network Programming Volume 1, Third Edition: The Sockets Networking ”，6.2节“I/O Models ”","link":"/2019/11/25/io-learning/"},{"title":"分布式链路追踪","text":"分布式链路追踪聊聊分布式链路追踪 各大厂分布式链路跟踪系统架构对比 什么是链路追踪分布式系统变得日趋复杂，越来越多的组件开始走向分布式化，如微服务、分布式数据库、分布式缓存等，使得后台服务构成了一种复杂的分布式网络。在服务能力提升的同时，复杂的网络结构也使问题定位更加困难。在一个请求在经过诸多服务过程中，出现了某一个调用失败的情况，查询具体的异常由哪一个服务引起的就变得十分抓狂，问题定位和处理效率是也会非常低。 分布式链路追踪就是将一次分布式请求还原成调用链路，将一次分布式请求的调用情况集中展示，比如各个服务节点上的耗时、请求具体到达哪台机器上、每个服务节点的请求状态等等。 Dapper目前业界的链路追踪系统，如Twitter的Zipkin，Uber的Jaeger，阿里的鹰眼，美团的Mtrace等都基本被启发于google发表的Dapper。 Dapper阐述了分布式系统，特别是微服务架构中链路追踪的概念、数据表示、埋点、传递、收集、存储与展示等技术细节。 Trace、Span、Annotations为了实现链路追踪，dapper提出了trace，span，annotation的概念。Trace的含义比较直观，就是链路，指一个请求经过后端所有服务的路径，可以用下面树状的图形表示。每一条链路都用一个全局唯一的traceid来标识。 分布式调用跟踪系统的设计（1）分布式调用跟踪系统的设计目标低侵入性，应用透明：作为非业务组件，应当尽可能少侵入或者无侵入其他业务系统，对于使用方透明，减少开发人员的负担 低损耗：服务调用埋点本身会带来性能损耗，这就需要调用跟踪的低损耗，实际中还会通过配置采样率的方式，选择一部分请求去分析请求路径 大范围部署，扩展性：作为分布式系统的组件之一，一个优秀的调用跟踪系统必须支持分布式部署，具备良好的可扩展性 （2）埋点和生成日志埋点即系统在当前节点的上下文信息，可以分为客户端埋点、服务端埋点，以及客户端和服务端双向型埋点。埋点日志通常要包含以下内容： TraceId、RPCId、调用的开始时间，调用类型，协议类型，调用方ip和端口，请求的服务名等信息； 调用耗时，调用结果，异常信息，消息报文等； 预留可扩展字段，为下一步扩展做准备； （3）抓取和存储日志日志的采集和存储有许多开源的工具可以选择，一般来说，会使用离线+实时的方式去存储日志，主要是分布式日志采集的方式。典型的解决方案如Flume结合Kafka等MQ。 （4）分析和统计调用链数据一条调用链的日志散落在调用经过的各个服务器上，首先需要按 TraceId 汇总日志，然后按照RpcId 对调用链进行顺序整理。用链数据不要求百分之百准确，可以允许中间的部分日志丢失。 （5）计算和展示汇总得到各个应用节点的调用链日志后，可以针对性的对各个业务线进行分析。需要对具体日志进行整理，进一步储存在HBase或者关系型数据库中，可以进行可视化的查询。","link":"/2020/01/07/j2ee-dapper/"},{"title":"j2ee-ioc","text":"IOCIOC，所谓控制反转，就是把原先我们代码里面需要实现的对象创建、依赖的代码，反转给容器来帮忙实现。 实现机制IOC(控制反转)的实现建立在工厂模式、Java反射机制 设计模式之单例模式 如何解决循环依赖123456789public class DefaultSingletonBeanRegistry extends SimpleAliasRegistry implements SingletonBeanRegistry { /** Cache of singleton objects: bean name --&gt; bean instance */ private final Map&lt;String, Object&gt; singletonObjects = new ConcurrentHashMap&lt;String, Object&gt;(256); /** Cache of singleton factories: bean name --&gt; ObjectFactory */ private final Map&lt;String, ObjectFactory&lt;?&gt;&gt; singletonFactories = new HashMap&lt;String, ObjectFactory&lt;?&gt;&gt;(16); /** Cache of early singleton objects: bean name --&gt; bean instance */ private final Map&lt;String, Object&gt; earlySingletonObjects = new HashMap&lt;String, Object&gt;(16); ​ 三级缓存，也就是三个Map集合类： singletonObjects：第一级缓存，里面放置的是已经实例化好的单例对象； earlySingletonObjects：第二级缓存，里面存放的是提前曝光的单例对象； singletonFactories：第三级缓存，里面存放的是将要被实例化的对象的对象工厂。 ​ 所以当一个Bean调用构造函数进行实例化后，即使set属性还未填充，就可以通过三级缓存向外暴露依赖的引用值进行set（所以循环依赖问题的解决也是基于Java的引用传递），这也说明了另外一点，基于构造函数的注入，如果有循环依赖，Spring是不能够解决的。 ​ 还要说明一点，Spring默认的Bean Scope是单例的，而三级缓存中都包含singleton，可见是对于单例Bean之间的循环依赖的解决，Spring是通过三级缓存来实现的。","link":"/2020/01/08/j2ee-ioc/"},{"title":"singleton单例模式","text":"不加锁实现饿汉模式实现单例使用static来定义静态成员变量或静态代码，借助Class的类加载机制实现线程安全单例。 12345678910111213public class Singleton { private static Singleton instance = new Singleton(); private Singleton (){} public static Singleton getInstance() { return instance; }} 通过静态内部类来实现1234567891011public class Singleton { private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton();} private Singleton (){} public static final Singleton getInstance() { return SingletonHolder.INSTANCE; }} 使用了lazy-loading。Singleton类被装载了，但是instance并没有立即初始化。因为SingletonHolder类没有被主动使用，只有显示通过调用getInstance方法时，才会显示装载SingletonHolder类，从而实例化instance。 使用枚举的方式123456public enum Singleton { INSTANCE; public void whateverMethod() {}} 这种方式是Effective Java作者Josh Bloch 提倡的方式，它不仅能避免多线程同步问题，而且还能防止反序列化重新创建新的对象，可谓是很坚强的壁垒。 面试官：以上几种答案，其实现原理都是利用借助了类加载的时候初始化单例。即借助了ClassLoader的线程安全机制。 所谓ClassLoader的线程安全机制，就是ClassLoader的loadClass方法在加载类的时候使用了synchronized关键字。也正是因为这样， 除非被重写，这个方法默认在整个装载过程中都是同步的，也就是保证了线程安全。 所以，以上各种方法，虽然并没有显示的使用synchronized，但是还是其底层实现原理还是用到了synchronized。 使用CASCAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 12345678910111213141516171819202122232425262728public class Singleton { private static final AtomicReference INSTANCE = new AtomicReference(); private Singleton() {} public static Singleton getInstance() { for (;;) { Singleton singleton = INSTANCE.get(); if (null != singleton) { return singleton; } singleton = new Singleton(); if (INSTANCE.compareAndSet(null, singleton)) { return singleton; } } }} 用CAS的好处在于不需要使用传统的锁机制来保证线程安全,CAS是一种基于忙等待的算法,依赖底层硬件的实现,相对于锁它没有线程切换和阻塞的额外消耗,可以支持较大的并行度。 CAS的一个重要缺点在于如果忙等待一直执行不成功(一直在死循环中),会对CPU造成较大的执行开销。 另外，如果N个线程同时执行到singleton = new Singleton();的时候，会有大量对象创建，很可能导致内存溢出。 加锁实现","link":"/2019/12/31/j2ee-model-singleton/"},{"title":"如何保证系统的稳定性和高可用","text":"如何保证系统的稳定性和高可用##现象分析 ​ 在很长时间里，我们做的只是在原有系统上增加功能，架构上没有大调整。但是随着业务增长，就算我们系统没有任何发版升级，也会突然出现一些事故。 发现问题从大方面说就是：外部的问题和自身的问题 ​ 值得一提的是：如果我们业务量没有上来，这些问题本不是问题。 ##故障演练 checklist 参照： 不可容忍的4个9，业务高速增长下，如何保证系统的稳定性和高可用？","link":"/2020/01/10/j2ee-stable/"},{"title":"j2ee项目多数据源配置","text":"简介个别情况下，项目中需要使用到连接多个不同的数据库的操作，本文就基于Spring boot配置多个数据源如何进行方便的配置进行梳理配置文件yaml文件12345678910spring: datasource: initialize: false url: ip:port/A username: projectuser1 password: *** datasource2: url: ip:port/B username: projectuser2 password: *** config文件​ 这里需要建立多个独立的配置文件，用来隔离区分不同的包路径地址（最佳方案）。要注意线程切换db问题 123public enum DatabaseType { A, B // 这里的type，指的是db的名字} 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configuration@MapperScan(basePackages = \"com.zixcon.project1.dal.mapper\", sqlSessionFactoryRef = \"sqlSessionFactory\")public class DB1Config { @Value(\"${spring.datasource.url}\") private String dbUrl; @Value(\"${spring.datasource.username}\") private String username; @Value(\"${spring.datasource.password}\") private String password; @Bean(\"aDataSource\") @Primary public DataSource druidDataSource() throws Exception { DruidDataSource datasource = new DruidDataSource(); datasource.setUrl(\"jdbc:mysql://\" + dbUrl + \"?useUnicode=true&amp;characterEncoding=utf-8&amp;zeroDateTimeBehavior=convertToNull\"); datasource.setUsername(username); datasource.setPassword(password); datasource.setDriverClassName(\"com.mysql.jdbc.Driver\"); return datasource; } @Bean(\"sqlSessionFactory\") @Primary public SqlSessionFactory sqlSessionFactory(@Qualifier(\"aDataSource\") DataSource druidDataSource) throws Exception { SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); //mapper扫描相关,这里指定project1的db的xml文件路径 PathMatchingResourcePatternResolver resolver = new PathMatchingResourcePatternResolver();sqlSessionFactoryBean.setMapperLocations(resolver.getResources(\"classpath:mapper/**/*.xml\")); sqlSessionFactoryBean.setDataSource(druidDataSource); return sqlSessionFactoryBean.getObject(); } @Bean public DynamicDataSource dataSource(@Qualifier(\"aDataSource\") DataSource aDataSource,@Qualifier(\"bDataSource\") DataSource bDataSource) { Map&lt;Object, Object&gt; targetDataSources = new HashMap&lt;&gt;(); targetDataSources.put(DatabaseType.A, aDataSource); targetDataSources.put(DatabaseType.B, bDataSource); DynamicDataSource dataSource = new DynamicDataSource(); dataSource.setTargetDataSources(targetDataSources); dataSource.setDefaultTargetDataSource(aDataSource);// 默认的datasource设置 return dataSource; }} 12345678910111213141516171819202122232425262728@Configuration@MapperScan(basePackages = \"com.zixcon.project2.dal.mapper\", sqlSessionFactoryRef = \"bSqlSessionFactory\")public class DB2Config { @Value(\"${spring.datasource2.url}\") private String dbUrl2; @Value(\"${spring.datasource2.username}\") private String username2; @Value(\"${spring.datasource2.password}\") private String password2; @Bean(\"bDataSource\") public DataSource bDataSource() throws Exception { DruidDataSource datasource = new DruidDataSource(); ... } @Bean(\"bSqlSessionFactory\") public SqlSessionFactory sqlSessionFactory(@Qualifier(\"bDataSource\") DataSource collectionDataSource) throws Exception { SqlSessionFactoryBean sqlSessionFactoryBean = new SqlSessionFactoryBean(); sqlSessionFactoryBean.setDataSource(bDataSource); //mapper扫描相关,这里指定project2的db的xml文件路径 sqlSessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver() .getResources(\"classpath:comapper/**/*.xml\")); return sqlSessionFactoryBean.getObject(); }} 其中MapperScan指定代码包路径及sqlSessionFactoryRef，这个sqlSessionFactoryRef一定要指定且不能指错了，不然会报异常。 代码中创建SqlSessionFactory实例的时候，需要指定DataSource以及对应db的xml文件路径，这样才能把db和mybatis的执行文件关联起来，且这里对不同的db的xml文件进行了包及区分。 多数据源需要指定@Primary，保证在路径不适配的情况下采用的数据库连接策略走指定的这个。 需要实例化DynamicDataSource，确保线程上下文切换的过程中取得对应的db连接。否则A-B-A切换查询会报异常。","link":"/2019/12/16/j2ee-multi-db/"},{"title":"接口幂等","text":"概念幂等：任意多次执行所产生的影响均与一次执行的影响／效果相同 场景／方案唯一索引：防止新增脏数据。 常用于用户账户等 Token机制：防止页面重复提交。 原理上通过session token来实现的(也可以通过redis来实现)。当客户端请求页面时，服务器会生成一个随机数Token，并且将Token放置到session当中，然后将Token发给客户端（一般通过构造hidden表单）。下次客户端提交请求时，Token会随着表单一起提交到服务器端。 服务器端第一次验证相同过后，会将session中的Token值更新下，若用户重复提交，第二次的验证判断将失败，因为用户提交的表单中的Token没变，但服务器端session中Token已经改变了。 悲观锁: select * from table_xxx where id=’xxx’ for update; 悲观锁使用时一般伴随事务一起使用，数据锁定时间可能会很长，根据实际情况选用； 乐观锁: 乐观锁只是在更新数据那一刻锁表，其他时间不锁表，所以相对于悲观锁，效率更高。 通过版本号实现update table_xxx set name=#name#,version=version+1 where version=#version# 分布式锁: 通过第三方的系统(redis或zookeeper)，在业务系统插入数据或者更新数据，获取分布式锁，然后做操作，之后释放锁 某个长流程处理过程要求不能并发执行，可以在流程执行之前根据某个标志(用户ID+后缀等)获取分布式锁，其他流程执行时获取锁就会失败，也就是同一时间该流程只能有一个能执行成功，执行完成后，释放分布式锁 状态机幂等: 业务单据上面有个状态，状态在不同的情况下会发生变更，一般情况下存在有限状态机，这时候，如果状态机已经处于下一个状态，这时候来了一个上一个状态的变更，理论上是不能够变更的，这样的话，保证了有限状态机的幂等 对外提供接口的api如何保证幂等: [source来源，seq序列号] 如银联提供的付款接口：需要接入商户提交付款请求时附带：source来源，seq序列号；source+seq在数据库里面做唯一索引，防止多次付款(并发时，只能处理一个请求) 。重点：对外提供接口为了支持幂等调用，接口有两个字段必须传，一个是来源source，一个是来源方序列号seq，这个两个字段在提供方系统里面做联合唯一索引，这样当第三方调用时，先在本方系统里面查询一下，是否已经处理过，返回相应处理结果；没有处理过，进行相应处理，返回结果。注意，为了幂等友好，一定要先查询一下，是否处理过该笔业务，不查询直接插入业务系统，会报错，但实际已经处理了。 高并发下的幂等解决方案","link":"/2019/12/31/j2ee-%E5%B9%82%E7%AD%89/"},{"title":"java-thread-CompletableFuture","text":"CompletableFutureCompletableFuture 的执行模型 异步编程CompletableFuture实现高并发系统优化之请求合并","link":"/2020/01/07/java-thread-CompletableFuture/"},{"title":"java-threadlocal","text":"ThreadLocalThreadLocal用在什么地方？ThreadLocal归纳下来就2类用途： 保存线程上下文信息，在任意需要的地方可以获取！！！ ​ 常用的比如每个请求怎么把一串后续关联起来，就可以用ThreadLocal进行set，在后续的任意需要记录日志的方法里面进行get获取到请求id，从而把整个请求串起来。 线程安全的，避免某些情况需要考虑线程安全必须同步带来的性能损失！！！ ​ ThreadLocal为解决多线程程序的并发问题提供了一种新的思路。但是ThreadLocal也有局限性，我们来看看阿里规范： ​ 每个线程往ThreadLocal中读写数据是线程隔离，互相之间不会影响的，所以ThreadLocal无法解决共享对象的更新问题！ ThreadLocal一些细节！​ Thread类有属性变量threadLocals （类型是ThreadLocal.ThreadLocalMap），也就是说每个线程有一个自己的ThreadLocalMap ，所以每个线程往这个ThreadLocal中读写隔离的，并且是互相不会影响的。 ​ 12345678public void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } 12345678910111213141516171819202122232425262728293031static class ThreadLocalMap { /** * The entries in this hash map extend WeakReference, using * its main ref field as the key (which is always a * ThreadLocal object). Note that null keys (i.e. entry.get() * == null) mean that the key is no longer referenced, so the * entry can be expunged from table. Such entries are referred to * as \"stale entries\" in the code that follows. */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; } } /** * The initial capacity -- MUST be a power of two. */ private static final int INITIAL_CAPACITY = 16; /** * The table, resized as necessary. * table.length MUST always be a power of two. */ private Entry[] table;} ​ java对象的引用包括 ： 强引用，软引用，弱引用，虚引用 。 ​ 因为这里涉及到弱引用，简单说明下： ​ 弱引用也是用来描述非必需对象的，当JVM进行垃圾回收时，无论内存是否充足，该对象仅仅被弱引用关联，那么就会被回收。 ​ 当仅仅只有ThreadLocalMap中的Entry的key指向ThreadLocal的时候，ThreadLocal会进行回收的！！！ ​ ThreadLocal被垃圾回收后，在ThreadLocalMap里对应的Entry的键值会变成null，但是Entry是强引用，那么Entry里面存储的Object，并没有办法进行回收，所以ThreadLocalMap 做了一些额外的回收工作。 定位通过threadLocalHashCode来进行 1private final int threadLocalHashCode = nextHashCode(); 1234567ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } ThreadLocal的最佳实践！​ 由于线程的生命周期很长，如果我们往ThreadLocal里面set了很大很大的Object对象，虽然set、get等等方法在特定的条件会调用进行额外的清理，但是ThreadLocal被垃圾回收后，在ThreadLocalMap里对应的Entry的键值会变成null，但是后续在也没有操作set、get等方法了。 ​ 所以最佳实践，应该在我们不使用的时候，主动调用remove方法进行清理。 ​ 这里把ThreadLocal定义为static还有一个好处就是，由于ThreadLocal有强引用在，那么在ThreadLocalMap里对应的Entry的键会永远存在，那么执行remove的时候就可以正确进行定位到并且删除！！！ threadlocal讲解 进阶：FastThreadLocal​ 众所周知，JDK 中自带的 ThreadLocal 在线程池使用环境中，有内存泄漏的风险，很明显，Netty 为了避免这个 bug，重新进行了封装 ​ Netty 高性能之道 FastThreadLocal 源码分析（快且安全） ​ FastThreadLocal的关键是index，index可以理解成一个FastThreadLocal对象的唯一ID。这个index会在FastThreadLocalThread线程中，作为其indexedVariables属性（初始是一个Object[32]数组）的索引，达成与ThreadLocal类似的效果。 ​ 所以，在数量大的情况下，FastThreadLocal速度比ThreadLocal快的原因就在这里。","link":"/2020/01/08/java-threadlocal/"},{"title":"java容器","text":"同步容器Vector和HashTable​ 为了简化代码开发的过程，早期的JDK在java.util包中提供了Vector和HashTable两个同步容器，这两个容器的实现和早期的ArrayList和HashMap代码实现基本一样，\b不同在于Vector和HashTable在每个方法上都添加了synchronized关键字来保证同一个实例同时只有一个线程能访问。 ​ 通过对每个方法添加synchronized，保证了多次操作的串行。这种方式虽然使用起来方便了，但并没有解决高并发下的性能问题，与手动锁住ArrayList和HashMap并没有什么区别，不论读还是写都会锁住整个容器。其次这种方式存在另一个问题：当多个线程进行复合操作时，是线程不安全的。 并发容器CopyOnWrite和ConcurrentCopyOnWrite​ CopyOnWrite–写时复制容器是一种常用的并发容器，它通过多线程下读写分离来达到提高并发性能的目的，和前面我们讲解StampedLock时所用的解决方案类似：任何时候都可以进行读操作，写操作则需要加锁。不同的是，在CopyOnWrite中，对容器的修改操作加锁后，通过copy一个新的容器来进行修改，修改完毕后将容器替换为新的容器即可。 ​ 这种方式的好处显而易见：通过copy一个新的容器来进行修改，这样读操作就不需要加锁，可以并发读，因为在读的过程中是采用的旧的容器，即使新容器做了修改对旧容器也没有影响，同时也很好的解决了迭代过程中其他线程修改导致的并发问题。 JDK中提供的并发容器包括CopyOnWriteArrayList和CopyOnWriteArraySet ConcurrentHashMap​ ConcurrentHashMap容器相较于CopyOnWrite容器在并发加锁粒度上有了更大一步的优化，它通过修改对单个hash桶元素加锁的达到了更细粒度的并发控制。 ​ 通过ConcurrentHashMap添加元素的过程，知道了ConcurrentHashMap容器是通过CAS + synchronized一起来实现并发控制的。这里有个额外的问题：为什么使用synchronized而不使用ReentrantLock？前面我的文章也对synchronized以及ReentrantLock的实现方式和性能做过分析，在这里我的理解是synchronized在后期优化空间上比ReentrantLock更大。 ConcurrentSkipListMap​ java.util中对应的容器在java.util.concurrent包中基本都可以找到对应的并发容器：\bList和Set有对应的CopyOnWriteArrayList与CopyOnWriteArraySet\b\b，HashMap有对应的ConcurrentHashMap，但是有序的TreeMap或并没有对应的ConcurrentTreeMap。 ​ 为什么没有ConcurrentTreeMap呢？\b这是因为TreeMap内部使用了红黑树来实现，红黑树是一种自平衡的二叉树，当树被修改时，需要重新平衡，重新平衡操作可能会影响树的大部分节点，如果并发量非常大的情况下，这就需要在许多树节点上添加互斥锁，那并发就失去了意义。所以提供了另外一种并发下的有序map实现：ConcurrentSkipListMap。 ​ ConcurrentSkipListMap内部使用跳表（SkipList）这种数据结构来实现，他的结构相对红黑树来说非常简单理解，实现起来也相对简单，而且在理论上它的查找、插入、删除时间复杂度都为log(n)。在并发上，ConcurrentSkipListMap采用无锁的CAS+自旋来控制。 ​","link":"/2020/01/03/jdk-container/"},{"title":"锁","text":"死锁​ 死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。 ​ 死锁是多个线程在运行过程中互相竞争资源发生的僵局，若无外力作用，它们都无法推进下去。 ##死锁的四个必要条件 互斥条件：一个资源每次只能被一个进程使用。 请求和保持条件：一个进程因为请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已经获得的资源在没有使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 ##解决死锁的基本方法 预防死锁： 1）资源一次性分配：一次性分配所有资源，这样就不会有请求了（破坏请求条件） 2）只要有一个资源得不到分配，就不给这也进程分配其他的资源（破坏保持条件） 3）可剥夺资源：即当某进程获得了部分资源，但得不到其它资源，则释放已占有的资源（破坏不剥夺条件） 4）资源有序分配法：系统给每个资源赋予一个编号，每一个进程按照编程递增的顺序请求资源，释放则相反（破坏环路等待条件） 解除死锁： 常用的有： 1）剥夺资源：从其他进程剥夺足够数量的资源给死锁进程，以解除死锁状态； 2）撤销进程:可以直接撤销死锁进程或撤销代价最小的进程，直至有足够的资源可用；","link":"/2020/01/02/jdk-lock/"},{"title":"synchronized","text":"Synchronized锁升级：偏向锁 -&gt; 轻量级锁 -&gt; 重量级锁 死磕Synchronized底层实现 简介​ Java中提供了两种实现同步的基础语义：synchronized方法和synchronized块 ​ 被编译成class文件的时候，synchronized关键字和synchronized方法的字节码略有不同，我们可以用javap -v 命令查看class文件对应的JVM字节码信息: ​ 对于synchronized关键字而言: ​ javac在编译时，会生成对应的monitorenter和monitorexit指令分别对应synchronized同步块的进入和退出。 ​ 有两个monitorexit指令的原因是： ​ 为了保证抛异常的情况下也能释放锁，所以javac为同步代码块添加了一个隐式的try-finally，在finally中会调用monitorexit命令释放锁。 ​ 而对于synchronized方法而言: ​ javac为其生成了一个ACC_SYNCHRONIZED关键字，在JVM进行方法调用时，发现调用的方法被ACC_SYNCHRONIZED修饰，则会先尝试获得锁。 锁的几种形式传统的锁： ​ （也就是下文要说的重量级锁）依赖于系统的同步函数，在linux上使用mutex互斥锁，最底层实现依赖于futex，这些同步函数都涉及到用户态和内核态的切换、进程的上下文切换，成本较高。对于加了synchronized关键字但运行时并没有多线程竞争，或两个线程接近于交替执行的情况，使用传统锁机制无疑效率是会比较低的。 ​ 在JDK 1.6引入了两种新型锁机制：偏向锁和轻量级锁，它们的引入是为了解决在没有多线程竞争或基本没有竞争的场景下因使用传统锁机制带来的性能开销问题。 ​ ​ Java中的synchronized有偏向锁、轻量级锁、重量级锁三种形式，分别对应了锁只被一个线程持有、不同线程交替持有锁、多线程竞争锁三种情况。当条件不满足时，锁会按偏向锁-&gt;轻量级锁-&gt;重量级锁 的顺序升级。JVM种的锁也是能降级的，只不过条件很苛刻，不在我们讨论范围之内。","link":"/2020/01/09/jdk-synchronized/"},{"title":"jvm内存模型（jmm）","text":"JMM参照：深入理解JVM-内存模型（jmm）和GC 从cpu讲到内存模型:什么是java内存模型","link":"/2019/12/16/jvm-jmm/"},{"title":"jvm","text":"PermGen space和heap space区别12PermGen space说明方法区OOMheap space 堆中OOM jdk8真正开始废弃永久代，而使用元空间(Metaspace)","link":"/2019/12/13/jvm-pre/"},{"title":"linux三剑客之awk","text":"简介​ awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 ​ awk 是一种很棒的语言，它适合文本处理和报表生成，其语法较为常见，借鉴了某些语言的一些精华，如 C 语言等。在 linux 系统日常处理工作中，发挥很重要的作用，掌握了 awk将会使你的工作变的高大上。 awk 是三剑客的老大，利剑出鞘，必会不同凡响。 使用方法1`awk` `'{pattern + action}'` `{filenames}` ​ 尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。 ​ awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。 ​ 通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 ​ Linux三剑客之awk命令​","link":"/2020/01/03/linux-awk/"},{"title":"linux内存swap","text":"什么是linux的内存机制​ 我们知道，直接从物理内存读写数据要比从硬盘读写数据要快的多，因此，我们希望所有数据的读取和写入都在内存完成，而内存是有限的，这样就引出了物理内存与虚拟内存的概念。 ​ 物理内存就是系统硬件提供的内存大小，是真正的内存，相对于物理内存，在linux下还有一个虚拟内存的概念，虚拟内存就是为了满足物理内存的不足而提出的策略，它是利用磁盘空间虚拟出的一块逻辑内存，用作虚拟内存的磁盘空间被称为交换空间（Swap Space）。 ​ 作为物理内存的扩展，linux会在物理内存不足时，使用交换分区的虚拟内存，更详细的说，就是内核会将暂时不用的内存块信息写到交换空间，这样以来，物理内存得到了释放，这块内存就可以用于其它目的，当需要用到原始的内容时，这些信息会被重新从交换空间读入物理内存。 ​ Linux的内存管理采取的是分页存取机制，为了保证物理内存能得到充分的利用，内核会在适当的时候将物理内存中不经常使用的数据块自动交换到虚拟内存中，而将经常使用的信息保留到物理内存。 ​ 要深入了解linux内存运行机制，需要知道下面提到的几个方面： Linux系统会不时的进行页面交换操作，以保持尽可能多的空闲物理内存，即使并没有什么事情需要内存，Linux也会交换出暂时不用的内存页面。这可以避免等待交换所需的时间。 Linux 进行页面交换是有条件的，不是所有页面在不用时都交换到虚拟内存，linux内核根据”最近最经常使用“算法，仅仅将一些不经常使用的页面文件交换到虚拟 内存，有时我们会看到这么一个现象：linux物理内存还有很多，但是交换空间也使用了很多。其实，这并不奇怪，例如，一个占用很大内存的进程运行时，需 要耗费很多内存资源，此时就会有一些不常用页面文件被交换到虚拟内存中，但后来这个占用很多内存资源的进程结束并释放了很多内存时，刚才被交换出去的页面 文件并不会自动的交换进物理内存，除非有这个必要，那么此刻系统物理内存就会空闲很多，同时交换空间也在被使用，就出现了刚才所说的现象了。关于这点，不 用担心什么，只要知道是怎么一回事就可以了。 交换空间的页面在使用时会首先被交换到物理内存，如果此时没有足够的物理内存来容纳这些页 面，它们又会被马上交换出去，如此以来，虚拟内存中可能没有足够空间来存储这些交换页面，最终会导致linux出现假死机、服务异常等问题，linux虽 然可以在一段时间内自行恢复，但是恢复后的系统已经基本不可用了。 因此，合理规划和设计Linux内存的使用，是非常重要的. ​ 在Linux 操作系统中，当应用程序需要读取文件中的数据时，操作系统先分配一些内存，将数据从磁盘读入到这些内存中，然后再将数据分发给应用程序；当需要往文件中写 数据时，操作系统先分配内存接收用户数据，然后再将数据从内存写到磁盘上。然而，如果有大量数据需要从磁盘读取到内存或者由内存写入磁盘时，系统的读写性 能就变得非常低下，因为无论是从磁盘读数据，还是写数据到磁盘，都是一个很消耗时间和资源的过程，在这种情况下，Linux引入了buffers和 cached机制。 ​ buffers与cached都是内存操作，用来保存系统曾经打开过的文件以及文件属性信息，这样当操作系统需要读取某些文件时，会首先在buffers 与cached内存区查找，如果找到，直接读出传送给应用程序，如果没有找到需要数据，才从磁盘读取，这就是操作系统的缓存机制，通过缓存，大大提高了操 作系统的性能。但buffers与cached缓冲的内容却是不同的。 ​ buffers是用来缓冲块设备做的，它只记录文件系统的元数据（metadata）以及 tracking in-flight pages，而cached是用来给文件做缓冲。更通俗一点说：buffers主要用来存放目录里面有什么内容，文件的属性以及权限等等。 创建SWAP内存1234567891011创建2G的空间dd if=/dev/zero of=/root/swapfile1 bs=1M count=2048创建swap分区mkswap /root/swapfile1开启虚拟内存chmod 0600 /root/swapfile1swapon /root/swapfile1设定虚拟内存开机自动挂载echo &quot;/root/swapfile1 swap swapdefaults 0 0&quot; &gt;&gt; /etc/fstab查看虚拟内存信息free -m或者swapon -s以及ll -h 和df -h 取消SWAP内存123456取消 SWAP文件swapoff /root/swapfile1取消开机启动加载sed -i &quot;/swapfile1/d&quot; /etc/fstab删除 SWAP文件rm -rf /root/swapfile1","link":"/2019/11/14/linux%E5%86%85%E5%AD%98swap/"},{"title":"MQ学习（一）","text":"MQ的作用​ 消息队列中间件主要解决应用解耦，异步消息，流量削锋等问题 MQ带来的新问题​ 一般而言，引入的外部依赖越多，系统越脆弱。需要考虑消息的重复消费、消息丢失、保证消费顺序、数据一致性问题等 幂等及去重​ 幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的 ​ 去重策略：保证每条消息都有唯一编号(比如唯一流水号) ​ 方法1: 建立一个消息表，拿到这个消息做数据库的insert操作。给这个消息做一个唯一主键（primary key）或者唯一约束(uniq key)，那么就算出现重复消费的情况，就会导致主键冲突，那么就不再处理这条消息。 参见 ####如何保证消息的有序性 ​ 通过轮询所有队列的方式来确定消息被发送到哪一个队列（负载均衡策略）。订单号相同的消息会被先后发送到同一个队列中， ​ 在获取到路由信息以后，会根据算法来选择一个队列，同一个 OrderId 获取到的肯定是同一个队列。 消息队列如何保证顺序性？","link":"/2019/12/02/mq-pre/"},{"title":"in exists 区别","text":"​ 1、exists是对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率； 2、in是把外表和内表做hash连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。 3、如果用not in ，则是内外表都全表扫描，无索引，效率低，可考虑使用not exists，也可使用A left join B on A.id=B.id where B.id is null 进行优化。 MySQL中exists和in的区别及使用场景","link":"/2020/01/15/mysql-in-exists/"},{"title":"mysql索引","text":"聚簇索引与非聚簇索引定义​ 聚簇索引是对磁盘上实际数据重新组织以按指定的一个或多个列的值排序的算法。特点是存储数据的顺序和索引顺序一致。​ 一般情况下主键会默认创建聚簇索引，且一张表只允许存在一个聚簇索引。 ​ 在《数据库原理》一书中是这么解释聚簇索引和非聚簇索引的区别的：​ 聚簇索引的叶子节点就是数据节点，而非聚簇索引的叶子节点仍然是索引节点，只不过有指向对应数据块的指针。 综上，可以得出聚簇索引和非聚簇索引概念： 聚簇索引：即存储索引值，又存储行数据，数据文件和索引文件是同一个文件 非聚簇索引：索引文件和数据文件是独立分开的 ​ 那么如果数据表中没有主键呢？MySQL的解决办法是隐式地将一个唯一的非空的列定义为聚簇。那如果这也没有呢？MySQL就自己创建一个聚簇索引。 ​ 在Innodb中，二级索引除了存储本身的列值外，其叶子节点存储的不是‘行指针’，而是主键值，为什么是这样呢？原来这种方式在表结构发生变化的时候会有很大的优势。如果二级索引的存储顺序是以列值为基础的，那么在发生数据行的移动或者增加删除时候，必定会引起索引结构的巨大变化。 MyISAM与InnoDB​ mysql存储引擎MyISAM与InnoDB的底层数据结构的区别主要有，在磁盘上存储的文件以及存储索引以及组织存储索引的方式不同； MyISAM索引文件和数据文件是分离的(非聚集)，索引的叶节点存放的是对应索引在文件系统中的数据地址编码 innodb中，数据文件和索引文件是同一个文件，是聚簇索引。（注意区别普通索引） （1）InnoDB的主键采用聚簇索引存储，使用的是B+Tree作为索引结构，但是叶子节点存储的是索引值和数据本身（注意和MyISAM的不同）。（2）InnoDB的二级索引不使用聚蔟索引，叶子节点存储的是KEY字段加主键值。因此，通过二级索引查询首先查到是主键值，然后InnoDB再根据查到的主键值通过主键索引找到相应的数据块。（3）MyISAM的主键索引和二级索引叶子节点存放的都是列值与行号的组合，叶子节点中保存的是数据的物理地址（4）MyISAM引擎使用B+Tree作为索引结构，叶节点的data域存放的是数据记录的地址（5）为什么用B+Tree 不是BTree： B-Tree：如果一次检索需要访问4个节点，数据库系统设计者利用磁盘预读原理，把节点的大小设计为一个页，那读取一个节点只需要一次I/O操作，完成这次检索操作，最多需要3次I/O(根节点常驻内存)。数据记录越小，每个节点存放的数据就越多，树的高度也就越小，I/O操作就少了，检索效率也就上去了。 B+Tree：非叶子节点只存key，大大滴减少了非叶子节点的大小，那么每个节点就可以存放更多的记录，树更矮了，I/O操作更少了。所以B+Tree拥有更好的性能。 ​ 由于节子节点(数据页)只能按照一颗B+树排序，故一张表只能有一个聚簇索引。辅助索引的存在不影响聚簇索引中数据的组织，所以一张表可以有多个辅助索引 参照：mysql InnoDB index 主键采用聚簇索引，二级索引不采用聚簇索引 复合索引​ 复合索引也叫组合索引；用户可以在多个列上建立索引,这种索引叫做复合索引(组合索引)。 ​ 建了一个(a,b,c)的复合索引，那么实际等于建了(a),(a,b),(a,b,c)三个索引，因为每多一个索引，都会增加写操作的开销和磁盘空间的开销。查询是否使用该索引基于最左原则 ​ 同样的有复合索引（a,b,c），如果有如下的sql: select a,b,c from table where a=1 and b = 1。那么MySQL可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机io操作。减少io操作，特别的随机io其实是dba主要的优化策略。所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一 复合索引测试sql用例","link":"/2019/12/13/mysql-index/"},{"title":"mysql-join","text":"表级联方式在多表联合查询的时候，如果我们查看它的执行计划，就会发现里面有多表之间的连接方式。多表之间的连接有三种方式：Nested Loops，Hash Join 和 Sort Merge Join.具体适用哪种类型的连接取决于 当前的优化器模式 （ALL_ROWS 和 RULE） 取决于表大小 取决于连接列是否有索引 取决于连接列是否排序 HASH JOIN:散列连接​ Hash join散列连接是CBO 做大数据集连接时的常用方式，优化器使用两个表中较小的表（通常是小一点的那个表或数据源）利用连接键（JOIN KEY）在内存中建立散列表，将列数据存储到hash列表中，然后扫描较大的表，同样对JOIN KEY进行HASH后探测散列表，找出与散列表匹配的行。需要注意的是：如果HASH表太大，无法一次构造在内存中，则分成若干个partition，写入磁盘的temporary segment，则会多一个写的代价，会降低效率。 SORT MERGE JOIN:排序合并连接​ Merge Join 是先将关联表的关联列各自做排序，然后从各自的排序表中抽取数据，到另一个排序表中做匹配。 NESTED LOOP:嵌套循环连接​ Nested loops 工作方式是循环从一张表中读取数据(驱动表outer table)，然后访问另一张表（被查找表 inner table,通常有索引）。驱动表中的每一行与inner表中的相应记录JOIN。类似一个嵌套的循环。","link":"/2020/01/17/mysql-join/"},{"title":"零拷贝","text":"​ 零拷贝的应用程序要求内核（kernel）直接将数据从磁盘文件拷贝到套接字（Socket），而无须通过应用程序。零拷贝不仅提高了应用程序的性能，而且减少了内核和用户模式见上下文切换。 ​ 从文件中读取数据，并将数据传输到网络上的另一个程序的场景 #数据传输：传统方法 从磁盘中copy放到一个内存buf中，然后将buf通过socket传输给用户,下面是伪代码实现： 12read(file, tmp_buf, len);write(socket, tmp_buf, len); 从图中可以看出文件经历了4次copy过程： 1.首先，调用read方法，文件从user模式拷贝到了kernel模式；（用户模式-&gt;内核模式的上下文切换，在内部发送sys_read() 从文件中读取数据，存储到一个内核地址空间缓存区中） 2.之后CPU控制将kernel模式数据拷贝到user模式下；（内核模式-&gt; 用户模式的上下文切换，read()调用返回，数据被存储到用户地址空间的缓存区中） 3.调用write时候，先将user模式下的内容copy到kernel模式下的socket的buffer中（用户模式-&gt;内核模式，数据再次被放置在内核缓存区中，send（）套接字调用） 4.最后将kernel模式下的socket buffer的数据copy到网卡设备中；（send套接字调用返回） #数据传输：零拷贝方法 ​ 从传统的场景看，会注意到上图，第2次和第3次拷贝根本就是多余的。应用程序只是起到缓存数据被将传回到套接字的作用而已，别无他用。 数据可以直接从read buffer 读缓存区传输到套接字缓冲区，也就是省去了将操作系统的read buffer 拷贝到程序的buffer，以及从程序buffer拷贝到socket buffer的步骤，直接将read buffer拷贝到socket buffer。JDK NIO中的的transferTo() 方法就能够让您实现这个操作，这个实现依赖于操作系统底层的sendFile（）实现 1public void transferTo(long position, long count, WritableByteChannel target); linux 2.1 内核开始引入了sendfile函数，用于将文件通过socket传输。 1sendfile(socket, file, len); 通过sendfile传送文件只需要一次系统调用，当调用sendfile时： 1.首先通过DMA将数据从磁盘读取到kernel buffer中 2.然后将kernel buffer数据拷贝到socket buffer中 3.最后将socket buffer中的数据copy到网卡设备中（protocol buffer）发送； 这个不是真正的Zero-Copy sendfile与read/write模式相比，少了一次copy。但是从上述过程中发现从kernel buffer中将数据copy到socket buffer是没有必要的； Linux2.4 内核对sendfile做了改进，如图： 改进后的处理过程如下： 将文件拷贝到kernel buffer中；(DMA引擎将文件内容copy到内核缓存区) 向socket buffer中追加当前要发生的数据在kernel buffer中的位置和偏移量； 根据socket buffer中的位置和偏移量直接将kernel buffer的数据copy到网卡设备（protocol engine）中； 从图中看到，linux 2.1内核中的 “数据被copy到socket buffer”的动作，在Linux2.4 内核做了优化，取而代之的是只包含关于数据的位置和长度的信息的描述符被追加到了socket buffer 缓冲区中。DMA引擎直接把数据从内核缓冲区传输到协议引擎（protocol engine），从而消除了最后一次CPU copy。经过上述过程，数据只经过了2次copy就从磁盘传送出去了。这个才是真正的Zero-Copy(这里的零拷贝是针对kernel来讲的，数据在kernel模式下是Zero-Copy)。 NettyNetty的“零拷贝”主要体现在如下三个方面： 1) Netty的接收和发送ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。如果使用传统的堆内存（HEAP BUFFERS）进行Socket读写，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。相比于堆外直接内存，消息在发送过程中多了一次缓冲区的内存拷贝。 2) Netty提供了组合Buffer对象，可以聚合多个ByteBuffer对象，用户可以像操作一个Buffer那样方便的对组合Buffer进行操作，避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大的Buffer。 3) Netty的文件传输采用了transferTo方法，它可以直接将文件缓冲区的数据发送到目标Channel，避免了传统通过循环write方式导致的内存拷贝问题。","link":"/2020/01/06/netty-zerocopy/"},{"title":"reactor和preactor模型","text":"Reactor模型Reactor的核心思想： 将关注的I/O事件注册到多路复用器上，一旦有I/O事件触发，将事件分发到事件处理器中，执行就绪I/O事件对应的处理函数中。模型中有三个重要的组件： 多路复用器：由操作系统提供接口，Linux提供的I/O复用接口有select、poll、epoll； 事件分离器：将多路复用器返回的就绪事件分发到事件处理器中； 事件处理器：处理就绪事件处理函数。 Handle：标示文件描述符； Event Demultiplexer：执行多路事件分解操作，对操作系统内核实现I/O复用接口的封装；用于阻塞等待发生在句柄集合上的一个或多个事件（如select/poll/epoll）； Event Handler：事件处理接口； Event Handler A(B)：实现应用程序所提供的特定事件处理逻辑； Reactor：反应器，定义一个接口，实现以下功能： a)供应用程序注册和删除关注的事件句柄； b)运行事件处理循环； c)等待的就绪事件触发，分发事件到之前注册的回调函数上处理. 经典Reactor模式 在经典Reactor模式中，包含以下角色： Reactor ：将I/O事件发派给对应的Handler Acceptor ：处理客户端连接请求 Handlers ：执行非阻塞读/写 多工作线程Reactor模式 多Reactor的Reactor模式 Proactor模型​ 与Reactor不同的是，Proactor使用异步I/O系统接口将I/O操作托管给操作系统，Proactor模型中分发处理异步I/O完成事件，并调用相应的事件处理接口来处理业务逻辑。 Proactor类结构中包含有如下角色： Handle：用来标识socket连接或是打开文件； Async Operation Processor：异步操作处理器；负责执行异步操作，一般由操作系统内核实现； Async Operation：异步操作； Completion Event Queue：完成事件队列；异步操作完成的结果放到队列中等待后续使用； Proactor：主动器；为应用程序进程提供事件循环；从完成事件队列中取出异步操作的结果，分发调用相应的后续处理逻辑； Completion Handler：完成事件接口；一般是由回调函数组成的接口； Completion Handler A(B)：完成事件处理逻辑；实现接口定义特定的应用处理逻辑。 支持情况​ windows对异步I/O提供了非常好的支持，常用Proactor的模型实现服务器；而Linux对异步I/O操作(aio接口)的支持并不是特别理想，而且不能直接处理accept，因此Linux平台上还是以Reactor模型为主。 ​ 目前实现了纯异步操作的操作系统少，实现优秀的如windows IOCP，但由于其windows系统用于服务器的局限性，目前应用范围较小；而Unix/Linux系统对纯异步的支持有限，应用事件驱动的主流还是通过select/epoll来实现； java提供Asynchronous I/O相关接口以支持异步I/O，AIO有两种api进行操作： Future 方式 即提交一个 I/O 操作请求，返回一个 Future。然后您可以对 Future 进行检查，确定它是否完成，或者阻塞 IO 操作直到操作正常完成或者超时异常。 Callback 方式 即提交一个 I/O 操作请求，并且指定一个 CompletionHandler。当异步 I/O 操作完成时，便发送一个通知，此时这个 CompletionHandler 对象的 completed 或者 failed 方法将会被调用。 参照： https://www.jianshu.com/p/b4de9b85c79d","link":"/2019/11/25/reactor/"},{"title":"redis学习（一）","text":"简介REmote DIctionary Server(Redis) 是一个由SalvatoreSanfilippo写的key-value存储系统。 Redis是一个开源的使用ANSI C语言编写、遵守BSD协议、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 它通常被称为数据结构服务器，因为值（value）可以是字符串(String), 哈希(Map), 列表(list), 集合(sets) 和有序集合(sorted sets)等类型。 Redis 与其他 key - value 缓存产品有以下三个特点： Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 redis常用数据类型 String Hash List Set Sorted set redis其他数据类型​ Bitmap:位图是支持按 bit 位来存储信息，可以用来实现 布隆过滤器（BloomFilter）； ​ HyperLogLog:供不精确的去重计数功能，比较适合用来做大规模数据的去重统计，例如统计 UV； ​ Geospatial:可以用来保存地理位置，并作位置距离计算或者根据半径计算位置等。 ​ … ​ Pipeline：执行批量指令，一次性返回全部结果，可以减少频繁的请求应答。 ​ Lua：脚本，可以执行一系列的功能。利用他的原子性。 ​ 事物：Redis 提供的不是严格的事务，只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。 redis常用命令 Key（键） String Hash List Set Sorted set pub/sub（发布／订阅） Transactions（事物） Script（脚本） Conection（链接） Server（服务器） 后面五个不常用，具体命令操作参见地址： 官方 翻译 redis持久化（RDB／AOF） RDB 持久化可以在指定的时间间隔内生成数据集的时间点快照（point-in-time snapshot）。 AOF 持久化记录服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。 AOF 文件中的命令全部以 Redis 协议的格式来保存，新命令会被追加到文件的末尾。 Redis 还可以在后台对 AOF 文件进行重写（rewrite），使得 AOF 文件的体积不会超出保存数据集状态所需的实际大小。 Redis 还可以同时使用 AOF 持久化和 RDB 持久化。 在这种情况下， 当 Redis 重启时， 它会优先使用 AOF 文件来还原数据集， 因为 AOF 文件保存的数据集通常比 RDB 文件所保存的数据集更完整。 你甚至可以关闭持久化功能，让数据只在服务器运行时存在。 缺点： RDB： 1.设置不同的保存点（save point）来控制保存 RDB 文件的频率，一旦发生故障可能会丢失数据。 2.每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； AOF： 1.相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 2…. 优点： RDB： 1.保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份 2.速度快，无论保存还是恢复 AOF： 1.非常耐久（much more durable） 2.AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek 3.在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 4.AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 setex和setnx命令区别 123456789SETEX key seconds value将值 value 关联到 key ，并将 key 的生存时间设为 seconds (以秒为单位)。如果 key 已经存在， SETEX 命令将覆写旧值。 这个命令类似于以下两个命令： SET key value EXPIRE key seconds # 设置生存时间不同之处是， SETEX 是一个原子性(atomic)操作，关联值和设置生存时间两个动作会在同一时间内完成，该命令在 Redis 用作缓存时，非常实用。 1234SETNX key value将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。 Redis 事物redis 数据库一致性数据淘汰策略redis 提供 6种数据淘汰策略： volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key（这个是最常用的）. allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。 TTL设置过期时间，Redis如何对这批key进行删除？ ​ 定期删除+惰性删除 ​ 定期删除，指的是redis默认是每隔100ms就随机抽取一些设置了过期时间的key，随机抽取一些key来检查和删除的。（全部检查，那redis基本上就死了，cpu负载会很高的，消耗在你的检查过期key上了。） ​ 问题是，定期删除可能会导致很多过期key到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个key的时候，redis会检查一下 ，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。","link":"/2019/11/26/redis-learning-1/"},{"title":"redis学习（三）","text":"#缓存的击穿 穿透 和 雪崩 ####缓存雪崩 ​ 缓存雪崩是指在我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到，瞬时压力过重雪崩。或在大部分数据都没缓存的时候被一次性同时访问。 解决方案： 缓存失效时的雪崩效应对底层系统的冲击非常可怕。我们可以用以下方式来避免： 1.数据预热： 可以通过缓存reload机制，预先去更新缓存，再即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀 2.做二级缓存，或者双缓存策略： A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期。 缓存击穿​ 对于一些设置了过期时间的key，如果这些key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据，这个时候，需要考虑一个问题：缓存被“击穿”的问题。 ​ 这个和缓存雪崩的区别在于这里针对某一key缓存，前者则是很多key。缓存在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 解决方案： 1.使用互斥锁(mutex key) 业界比较常用的做法，是使用mutex。简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如mutex key，当操作返回成功时，再进行Redis的SETNX或者Memcache的ADD）去set一个的操作并回设缓存；否则，就重试整个get缓存的方法。 1234567891011121314151617181920public String get(key) { String value = redis.get(key); if (value == null) { //代表缓存值过期 //设置3min的超时，防止del操作失败的时候，下次缓存过期一直不能load db if (redis.setnx(key_mutex, 1, 3 * 60) == 1) { //代表设置成功 value = db.get(key); redis.set(key, value, expire_secs); redis.del(key_mutex); } else { //这个时候代表同时候的其他线程已经load db并回设到缓存了，这时候重试获取缓存值即可 sleep(50); get(key); //重试 } } else { return value; }} ​ 2.设置热点数据永远不过期 ​ 这里的“永远不过期”包含两层意思：​ 1)从缓存上看，确实没有设置过期时间，这就保证了，不会出现热点key过期问题，也就是“物理”不过期。​ 2)从功能上看，如果不过期，那不就成静态的了吗？所以我们把过期时间存在key对应的里，如果发现要过期了，通过一个后台的异步线程进行缓存的构建，也就是“逻辑”过期. ​ 4.用主从模式和集群模式来尽量保证缓存服务的高可用。 ​ 使用场景：比如首页排名列表 缓存穿透​ 缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。在流量大时，可能DB就挂掉了，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。 解决方案： 有很多种方法可以有效地解决缓存穿透问题， ​ 1.最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。 ​ 2.另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。 ​ 3.Nginx对单个IP每秒访问次数进行限制 一般避免以上情况发生我们从三个时间段去分析下： 事前：Redis 高可用，主从+哨兵，Redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + Hystrix 限流+降级，避免** MySQL** 被打死。 事后：Redis 持久化 RDB+AOF，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空白的值。 好处： ​ 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，3/5 的请求都是可以被处理的。 只要有 3/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来一次。 ​ 这个在目前主流的互联网大厂里面是最常见的 参见：敖丙的系列 设计缓存系统：缓存穿透，缓存击穿，缓存雪崩解决方案分析","link":"/2019/11/26/redis-learning-3/"},{"title":"redis分布式锁","text":"redis能用的的加锁命令分表是INCR、SETNX、SET 第一种锁命令INCR这种加锁的思路是， key 不存在，那么 key 的值会先被初始化为 0 ，然后再执行 INCR 操作进行加一。然后其它用户在执行 INCR 操作进行加一时，如果返回的数大于 1 ，说明这个锁正在被使用当中。 1、 客户端A请求服务器获取key的值为1表示获取了锁 2、 客户端B也去请求服务器获取key的值为2表示获取锁失败 3、 客户端A执行代码完成，删除锁 4、 客户端B在等待一段时间后在去请求的时候获取key的值为1表示获取锁成功 5、 客户端B执行代码完成，删除锁 12$redis-&gt;incr($key);$redis-&gt;expire($key, $ttl); //设置生成时间为1秒 ####第二种锁SETNX 这种加锁的思路是，如果 key 不存在，将 key 设置为 value 如果 key 已存在，则 SETNX 不做任何动作 1、 客户端A请求服务器设置key的值，如果设置成功就表示加锁成功 2、 客户端B也去请求服务器设置key的值，如果返回失败，那么就代表加锁失败 3、 客户端A执行代码完成，删除锁 4、 客户端B在等待一段时间后在去请求设置key的值，设置成功 5、 客户端B执行代码完成，删除锁 12$redis-&gt;setNX($key, $value);$redis-&gt;expire($key, $ttl); ####第三种锁SET 上面两种方法都有一个问题，会发现，都需要设置 key 过期。那么为什么要设置key过期呢？如果请求执行因为某些原因意外退出了，导致创建了锁但是没有删除锁，那么这个锁将一直存在，以至于以后缓存再也得不到更新。于是乎我们需要给锁加一个过期时间以防不测。 但是借助 Expire 来设置就不是原子性操作了。所以还可以通过事务来确保原子性，但是还是有些问题，所以官方就引用了另外一个，使用 SET 命令本身已经从版本 2.6.12 开始包含了设置过期时间的功能。 1、 客户端A请求服务器设置key的值，如果设置成功就表示加锁成功 2、 客户端B也去请求服务器设置key的值，如果返回失败，那么就代表加锁失败 3、 客户端A执行代码完成，删除锁 4、 客户端B在等待一段时间后在去请求设置key的值，设置成功 5、 客户端B执行代码完成，删除锁 1$redis-&gt;set($key, $value, array(&apos;nx&apos;, &apos;ex&apos; =&gt; $ttl)); //ex表示秒 ####其它问题 虽然上面一步已经满足了我们的需求，但是还是要考虑其它问题？ 1、 redis发现锁失败了要怎么办？中断请求还是循环请求？ 2、 循环请求的话，如果有一个获取了锁，其它的在去获取锁的时候，是不是容易发生抢锁的可能？ 3、 锁提前过期后，客户端A还没执行完，然后客户端B获取到了锁，这时候客户端A执行完了，会不会在删锁的时候把B的锁给删掉？ ####解决办法 针对问题1：使用循环请求，循环请求去获取锁 针对问题2：针对第二个问题，在循环请求获取锁的时候，加入睡眠功能，等待几毫秒在执行循环 针对问题3：在加锁的时候存入的key是随机的。这样的话，每次在删除key的时候判断下存入的key里的value和自己存的是否一样 12345678910111213141516do { //针对问题1，使用循环 $timeout = 10; $roomid = 10001; $key = &apos;room_lock&apos;; $value = &apos;room_&apos;.$roomid; //分配一个随机的值针对问题3 $isLock = Redis::set($key, $value, &apos;ex&apos;, $timeout, &apos;nx&apos;);//ex 秒 if ($isLock) { if (Redis::get($key) == $value) { //防止提前过期，误删其它请求创建的锁 //执行内部代码 Redis::del($key); continue;//执行成功删除key并跳出循环 } } else { usleep(5000); //睡眠，降低抢锁频率，缓解redis压力，针对问题2 }} while(!$isLock); ####另外一个锁 以上的锁完全满足了需求，但是官方另外还提供了一套加锁的算法，这里以PHP为例 12345678910111213$servers = [ [&apos;127.0.0.1&apos;, 6379, 0.01], [&apos;127.0.0.1&apos;, 6389, 0.01], [&apos;127.0.0.1&apos;, 6399, 0.01],];$redLock = new RedLock($servers);//加锁$lock = $redLock-&gt;lock(&apos;my_resource_name&apos;, 1000);//删除锁$redLock-&gt;unlock($lock) 上面是官方提供的一个加锁方法，就是和第6的大体方法一样，只不过官方写的更健壮。所以可以直接使用官方提供写好的类方法进行调用。官方提供了各种语言如何实现锁。 引用：Redis实现加锁的几种方法示例","link":"/2019/12/24/redis-learning-lock/"},{"title":"spring学习前瞻","text":"Spring 源码分析文章列表Ⅰ. IOC 更新时间 标题 2018-05-30 Spring IOC 容器源码分析系列文章导读 2018-06-01 Spring IOC 容器源码分析 - 获取单例 bean 2018-06-04 Spring IOC 容器源码分析 - 创建单例 bean 的过程 2018-06-06 Spring IOC 容器源码分析 - 创建原始 bean 对象 2018-06-08 Spring IOC 容器源码分析 - 循环依赖的解决办法 2018-06-11 Spring IOC 容器源码分析 - 填充属性到 bean 原始对象 2018-06-11 Spring IOC 容器源码分析 - 余下的初始化工作 Ⅱ. AOP 更新时间 标题 2018-06-17 Spring AOP 源码分析系列文章导读 2018-06-20 Spring AOP 源码分析 - 筛选合适的通知器 2018-06-20 Spring AOP 源码分析 - 创建代理对象 2018-06-22 Spring AOP 源码分析 - 拦截器链的执行过程 Ⅲ. MVC 更新时间 标题 2018-06-29 Spring MVC 原理探秘 - 一个请求的旅行过程 2018-06-30 Spring MVC 原理探秘 - 容器的创建过程 参见：coolblog.xyz作者：coolblog.xyz转自：博客：http://www.coolblog.xyz","link":"/2019/12/04/spring-pre/"},{"title":"zk和eureka比较","text":"服务注册中心，Eureka与Zookeeper比较","link":"/2019/12/24/zk-eureka/"},{"title":"AOP原理及实现机制","text":"AOP原理及实现机制概念​ AOP即Aspect-Oriented Programming的缩写，中文意思是面向切面（或方面）编程。AOP实际上是一种编程思想，可以通过预编译方式和运行期动态代理实现在不修改源代码的情况下给程序动态统一添加功能的一种思想。 ​ 在传统的面向对象（Object-Oriented Progr amming，OOP）编程中，我们总是按照某种特定的执行顺序来实现业务流程，各个执行步骤之间是相互衔接、相互耦合的，对垂直切面关注度很高，横切面关注却很少，也很难关注。那么怎样可以解决这个问题呢？我们需要AOP，关注系统的“切面”，在适当的时候“拦截”程序的执行流程，把程序的预处理和后处理交给某个拦截器来完成。这样，业务流程就完全的从其它无关的代码中解放出来，各模块之间的分工更加明确，程序维护也变得容易多了。 ​ AOP主要的意图是：允许通过分离应用的业务逻辑与系统级服务进行内聚性的开发。应用对象只实现业务逻辑即可，并不负责其它的系统级关注点。 ​ AOP主要应用场景是：日志记录、跟踪、监控和优化，性能统计、优化，安全、权限控制，应用系统的异常捕捉及处理，事务处理，缓存，持久化，懒加载（Lazy loading），内容传递，调试，资源池，同步等等。 AOP原理和基本概念 原理​ AOP的发展目前已经历了两个阶段： ​ 第一阶–静态AOP ​ 第二阶段—动态AOP ​ 静态AOP阶段，相应的横切关注点以Aspect形式实现之后，会通过特定的编译器，将实现后的Aspect编译并织入到系统的静态类中。比如AspectJ会使用ajc编译器将各个Aspect以Java字节码的形式编译到系统的各个功能模块中，已达到融合Aspect和Class的目的。 ​ 动态AOP阶段，AOP的织入过程在系统运行开始之后进行，而不是预先编译到系统中，可以在调整织入点以及织入逻辑单元的同时，不必变更系统其他的模块，甚至在系统运行的时候，也可以动态更改织入逻辑。 ​ 这两个阶段也为我们提供了通过预编译方式和运行期动态代理方式，实现了在不修改源代码的情况下给程序统一添加功能的目的。 AOP的相关概念： Joinpoint：在系统运行之前，AOP的功能模块都需要织入到OOP的功能模块中。所以，要进行这种织入过程，我们需要知道在系统的那些执行点上进行织入操作。这些将要在其上进行织入操作的系统执行点就称之为Joinpoint。（在Spring AOP 中仅支持方法级别的Joinpoint） Pointcut：代表的是Joinpoint的表达方式。将横切逻辑织入当前系统的过程中，需要参照Pointcut规定的Joinpoint信息，才可以知道应该往系统的哪些Joinpoint上织入横切逻辑。 Pointcut指定系统中符合条件的一组Joinpoint。 Advice：是单一横切关注点逻辑的载体，代表将会织入到Joinpoint处的横切逻辑。如果将Aspect比作OOP中的Class，那么Advice就相当于Class中的Method。按照Advice在Joinpoint处执行时机的差异或者完成功能的不同，Advice可分成以下具体形式：Before Advice,After Advice(Afterreturning Advice、AfterThrowing Advice、AfterFinally Advice),Around Advice,Introduction。 Aspect**：**是对体统中的横切关注点逻辑进行模块化封装的AOP概念实体。Aspect可以包含多个Pointcut以及相应的Advice定义。 织入：完成横切关注点逻辑（以Aspect模块化的横切关注点）到OOP系统的过程。 实现机制​ AOP实现时有三种方式：生成子类字节码、生成代理类字节码、直接修改原类的字节码 类别 机制 原理 优点 缺点 静态AOP 静态织入 在编译期，切面直接以字节码的形式编译到目标字节码文件中。 对系统无性能影响。 灵活性不够。 动态AOP 动态代理 在运行期，目标类加载后，为接口动态生成代理类，将切面植入到代理类中。 相对于静态AOP更加灵活。 切入的关注点需要实现接口。对系统有一点性能影响。 动态字节码生成 在运行期，目标类加载后，动态构建字节码文件生成目标类的子类，将切面逻辑加入到子类中。 没有接口也可以织入。 扩展类的实例方法为final时，则无法进行织入。 自定义类加载器 在运行期，目标加载前，将切面逻辑加到目标字节码里。 可以对绝大部分类进行织入。 代码中如果使用了其他类加载器，则这些类将不会被织入。 字节码转换 在运行期，所有类加载器加载字节码前，前进行拦截。 可以对所有类进行织入。 Java实现AOP机制 动态代理 ​ 使用动态代理实现AOP需要有四个角色：被代理的类，被代理类的接口，织入器，和InvocationHandler。 ​ 主要通过代理模式实现，为实例对象通过代理模式创建了代理对象，访问这些实例对象必须要通过代理。 ​ 动态代理在运行期通过接口动态生成代理类，这为其带来了一定的灵活性，但这个灵活性却带来了两个问题，第一代理类必须实现一个接口，如果没实现接口会抛出一个异常。第二性能影响，因为动态代理使用反射的机制实现的，首先反射肯定比直接调用要慢 代表：JDK 动态字节码生成 ​ 在运行期间目标字节码加载后，生成目标类的子类，将切面逻辑加入到子类中，所以使用Cglib实现AOP不需要基于接口。 代表：Cglib 自定义类加载器 ​ 自定义类加载器，在类加载到JVM之前直接修改某些类的方法，并将切入逻辑织入到这个方法里，然后将修改后的字节码文件交给虚拟机运行​ Javassist是一个编辑字节码的框架，可以让你很简单地操作字节码。它可以在运行期定义或修改Class。使用Javassist实现AOP的原理是在字节码加载前直接修改需要切入的方法。这比使用Cglib实现AOP更加高效，并且没太多限制。 ​ 自定义的类加载器实现AOP在性能上要优于动态代理和Cglib，因为它不会产生新类，但是它仍然存在一个问题，就是如果其他的类加载器来加载类的话，这些类将不会被拦截。 代表：Javassist 其他方式 ​ 自定义的类加载器实现AOP只能拦截自己加载的字节码，那么有没有一种方式能够监控所有类加载器加载字节码呢？有，使用Instrumentation，它是 Java 5 提供的新特性，使用 Instrumentation，开发者可以构建一个字节码转换器，在字节码加载前进行转换。 ###Spring的AOP ​ Spring默认采取的动态代理机制实现AOP，当动态代理不可用时（代理类无接口）会使用CGlib机制。 ​ AOP(面向方面编程)的实现是建立在CGLib提供的类代理和jdk提供的接口代理 使用场景 性能监控，在方法调用前后记录调用时间，方法执行太长或超时报警。 缓存代理，缓存某方法的返回值，下次执行该方法时，直接从缓存里获取。 软件破解，使用AOP修改软件的验证类的判断逻辑。 记录日志，在方法执行前后记录系统日志。 工作流系统，工作流系统需要将业务代码和流程引擎代码混合在一起执行，那么我们可以使用AOP将其分离，并动态挂接业务。 权限验证，方法执行前验证是否有权限执行当前方法，没有则抛出没有权限执行异常，由业务代码捕捉。 ​","link":"/2019/12/25/j2ee-aop/"},{"title":"LOG日志桥接关系","text":"这里思考几个问题： 1.日志框架是给Java应用提供的方便进行记录日志的，那为什么又不让在应用中直接使用其API呢？ 2.这里面推崇使用的SLF4J是什么呢？ 3.所谓的门面模式又是什么东西呢？ 4.它是怎么做到统一的呢？ 门面模式迪米特法则：talk only to your immediate friends 迪米特法则的[初衷](https://baike.baidu.com/item/初衷)在于降低类之间的[耦合](https://baike.baidu.com/item/耦合/2821124)。由于每个类尽量减少对其他类的依赖，因此，很容易使得系统的功能模块功能独立，相互之间不存在（或很少有）依赖关系。 迪米特法[友元类](https://baike.baidu.com/item/友元类/518734)转达。因此，应用迪米特法则有可能造成的一个后果就是：系统中存在大量的中介类，这些类之所以存在完全是为了传递类之间的相互调用关系这在一定程度上增加了系统的复杂度。 ​ 外观模式创造出一个外观对象，将客户端所涉及的属于一个子系统的协作伙伴的数量减到最少，使得客户端与子系统内部的对象的相互作用被外观对象所取代。外观类充当了客户类与子系统类之间的“第三者”，降低了客户类与子系统类之间的耦合度 更形象点： 优点: - 松耦合 用户与子系统解耦，屏蔽子系统；可以提高子系统的独立性； - 简单易用 简化用户与子系统的依赖关系； 用户只与门面对接，有统一的入口；不需要知道所有子系统及内部构造； - 更好的划分访问层次 有些方法是对系统外的，有些方法是系统内部相互交互的使用的。子系统把那些暴露给外部的功能集中到门面中，这样就可以实现客户端的使用，很好的隐藏了子系统内部的细节。 缺点: 不能很好地限制客户使用子系统类，如果对客户访问子系统类做太多的限制则减少了可变性和灵活性。 在不引入抽象外观类的情况下，增加新的子系统可能需要修改外观类或客户端的源代码，违背了“开闭原则”。开闭原则两个主要特征： （1）对于扩展是开放的（Open for extension）。 （2）对于修改是关闭的（Closed for modification）。 错误使用行为 不要通过继承一个外观类在子系统中加入新的行为，这种做法是错误的。 外观模式的用意是为子系统提供一个集中化和简化的沟通渠道，而不是向子系统加入新的行为，新的行为的增加应该通过修改原有子系统类或增加新的子系统类来实现，不能通过外观类来实现。Log日志框架关系 1996年早期，欧洲安全电子市场项目组决定编写它自己的程序跟踪API(Tracing API)。经过不断的完善，这个API终于成为一个十分受欢迎的Java日志软件包，即Log4j。后来Log4j成为Apache基金会项目中的一员。 期间Log4j近乎成了Java社区的日志标准。据说Apache基金会还曾经建议Sun引入Log4j到java的标准库中，但Sun拒绝了。 2002年Java1.4发布，Sun推出了自己的日志库JUL(Java Util Logging),其实现基本模仿了Log4j的实现。在JUL出来以前，Log4j就已经成为一项成熟的技术，使得Log4j在选择上占据了一定的优势。 接着，Apache推出了Jakarta Commons Logging，JCL只是定义了一套日志接口(其内部也提供一个Simple Log的简单实现)，支持运行时动态加载日志组件的实现，也就是说，在你应用代码里，只需调用Commons Logging的接口，底层实现可以是Log4j，也可以是Java Util Logging。 后来(2006年)，Ceki Gülcü离开了Apache。然后先后创建了Slf4j(日志门面接口，类似于Commons Logging)和Logback(Slf4j的实现)两个项目，并回瑞典创建了QOS公司，QOS官网上是这样描述Logback的：The Generic，Reliable Fast&amp;Flexible Logging Framework(一个通用，可靠，快速且灵活的日志框架)。从此以后，Java日志领域被划分为两大阵营：Commons Logging阵营和Slf4j阵营。 2012年，Apache重写了Log4j 1.x，成立了新的项目Log4j 2 Commons Logging和Slf4j是日志门面 Log4j和Logback则是具体的日志实现方案 这里可以看出分为三个层次： 抽象层：提供应用直接调用api 适配层：提供于抽象层和实现层的适配实现 实现层：api的实现 通过log4j-slf4j-impl来看是如何桥接 jar包名 说明 slf4j-log4j12-1.7.13.jar Log4j1.2版本的桥接器，你需要将Log4j.jar加入Classpath。 slf4j-jdk14-1.7.13.jar java.util.logging的桥接器，Jdk原生日志框架。 slf4j-nop-1.7.13.jar NOP桥接器，默默丢弃一切日志。 slf4j-simple-1.7.13.jar 一个简单实现的桥接器，该实现输出所有事件到System.err. 只有Info以及高于该级别的消息被打印，在小型应用中它也许是有用的。 slf4j-jcl-1.7.13.jar Jakarta Commons Logging 的桥接器. 这个桥接器将Slf4j所有日志委派给Jcl。 logback-classic-1.0.13.jar(requires logback-core-1.0.13.jar) Slf4j的原生实现，Logback直接实现了Slf4j的接口，因此使用Slf4j与Logback的结合使用也意味更小的内存与计算开销 如何将日志框架指向SLF4J： jar包名 作用 log4j-over-slf4j.jar 将Log4j 1重定向到Slf4j jcl-over-slf4j.jar 将Commons Logging里的Simple Logger重定向到slf4j jul-to-slf4j.jar 将Java Util Logging重定向到Slf4j log4j-to-slf4j log4j 2 API到slf4j的适配 Slf4j/log4j2集成举例 如果我们在系统中需要使用slf4j和log4j2来进行日志输出的话，我们需要引入下面的jar包： 123log4j2核心jar包：log4j-api-2.7.jar和log4j-core-2.7.jarslf4j核心jar包：slf4j-api-1.6.4.jarslf4j与log4j2的桥接包：log4j-slf4j-impl-2.7.jar，这个包的作用就是使用slf4j的api，但是底层实现是基于log4j2 死循环问题 多个日志jar包形成死循环的条件 产生原因 log4j-over-slf4j.jar和slf4j-log4j12.jar 由于slf4j-log4j12.jar的存在会将所有日志调用委托给log4j。但由于同时由于log4j-over-slf4j.jar的存在，会将所有对log4j api的调用委托给相应等值的slf4j,所以log4j-over-slf4j.jar和slf4j-log4j12.jar同时存在会形成死循环 jcl-over-slf4j 与 slf4j-jcl Jcl-over-slf4j : commons-logging切换到slf4j slf4j-jcl : slf4j切换到commons-logging jul-to-slf4j.jar和slf4j-jdk14.jar 由于slf4j-jdk14.jar的存在会将所有日志调用委托给jdk的log。但由于同时jul-to-slf4j.jar的存在，会将所有对jul api的调用委托给相应等值的slf4j，所以jul-to-slf4j.jar和slf4j-jdk14.jar同时存在会形成死循环 123死循环日志：SLF4J: Detected both log4j-over-slf4j.jar AND slf4j-log4j12.jar on the class path, preempting StackOverflowError. SLF4J: See also http://www.slf4j.org/codes.html#log4jDelegationLoop for more details. 看到这里，是不是很头大，这么多工具类，怎么管理这些包之间的关系，使得日志的使用情况和预期一致，并且规避日志引起的问题呢？ 日志依赖包管理方案方案1:采用maven的exclusion方案 优点是exclusion是maven原生提供的 不足之处是如果有多个组件都依赖了commons-logging，则需要在很多处增加，使用起来不太方便 方案2: 在maven声明commons-logging的scope为provided ​ 这种方案在调试代码时还是有可能导致IDE将commons-logging放置在classpath下，从而导致程序运行时出现异常 方案3:在maven私服中增加虚拟的版本号 【建议方案】 ​ 好处是声明方式比较简单，用IDE调试代码时也不会出现问题，不足之处是maven中央仓库中是不存在的，需要发布到自己的maven私服中。 ​ 但是，这种方式得注意，由于maven的加载关系，必须得把此配置声明放在顶级pom文件中，保证优先加载此空jar包 虚拟版本号原理 123Maven 解析 pom.xml 文件时，同一个 jar 包只会保留一个，这样有效的避免因引入两个 jar 包导致的工程运行不稳定性。 举例：假设 A-&gt;B-&gt;C-&gt;D1, E-&gt;F-&gt;D2，D1,D2 分别为 D 的不同版本。那么会引入哪个版本？ Maven 默认处理策略 最短路径优先 Maven 面对 D1 和 D2 时，会默认选择最短路径的那个 jar 包，即 D2。E-&gt;F-&gt;D2 比 A-&gt;B-&gt;C-&gt;D1 路径短 1。 最先声明优先 ​ 如果路径一样的话，举个： A-&gt;B-&gt;C1, E-&gt;F-&gt;C2 ，两个依赖路径长度都是 2，那么就选择最先声明。","link":"/2019/11/29/j2ee-log/"},{"title":"redis学习（二）","text":"参见：文章地址 Redis对象类型简介 Redis是一种key/value型数据库，其中，每个key和value都是使用对象表示的。比如，我们执行以下代码： 1redis＞ SET message &quot;hello redis&quot; 其中的key是message，是一个包含了字符串”message”的对象。而value是一个包含了”hello redis”的对象。Redis共有五种对象的类型，分别是： 类型常量 对象的名称 REDIS_STRING 字符串对象 REDIS_LIST 列表对象 REDIS_HASH 哈希对象 REDIS_SET 集合对象 REDIS_ZSET 有序集合对象 Redis中的一个对象的结构体表示如下： 123456789101112131415typedef struct redisObject { // 类型 unsigned type:4; // 编码方式 unsigned encoding: 4; // 引用计数 int refcount; // 指向对象的值 void *ptr; } robj; type表示了该对象的对象类型，即上面五个中的一个。但为了提高存储效率与程序执行效率，每种对象的底层数据结构实现都可能不止一种。encoding就表示了对象底层所使用的编码。 Redis对象底层数据结构 编码常量 编码所对应的底层数据结构 REDIS_ENCODING_INT long 类型的整数 REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 REDIS_ENCODING_RAW 简单动态字符串 REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 字符串对象 字符串对象的编码可以是int、raw或者embstr如果一个字符串的内容可以转换为long，那么该字符串就会被转换成为long类型，对象的ptr就会指向该long，并且对象类型也用int类型表示。普通的字符串有两种，embstr和raw。embstr应该是Redis 3.0新增的数据结构,在2.8中是没有的。如果字符串对象的长度小于39字节，就用embstr对象。否则用传统的raw对象。 1234567#define REDIS_ENCODING_EMBSTR_SIZE_LIMIT 44 robj *createStringObject(char *ptr, size_t len) { if (len &lt;= REDIS_ENCODING_EMBSTR_SIZE_LIMIT) return createEmbeddedStringObject(ptr,len); else return createRawStringObject(ptr,len); } embstr的好处有如下几点： embstr的创建只需分配一次内存，而raw为两次（一次为sds分配对象，另一次为objet分配对象，embstr省去了第一次）。 相对地，释放内存的次数也由两次变为一次。 embstr的objet和sds放在一起，更好地利用缓存带来的优势。 raw和embstr的区别可以用下面两幅图所示： 列表对象 列表对象的编码可以是ziplist或者linkedlist ziplist是一种压缩链表，它的好处是更能节省内存空间，因为它所存储的内容都是在连续的内存区域当中的。当列表对象元素不大，每个元素也不大的时候，就采用ziplist存储但当数据量过大时就ziplist就不是那么好用了。因为为了保证他存储内容在内存中的连续性，插入的复杂度是O(N)，即每次插入都会重新进行realloc。如下图所示，对象结构中ptr所指向的就是一个ziplist整个ziplist只需要malloc一次，它们在内存中是一块连续的区域。 linkedlist是一种双向链表。它的结构比较简单，节点中存放pre和next两个指针，还有节点相关的信息。当每增加一个node的时候，就需要重新malloc一块内存。 哈希对象 哈希对象的底层实现可以是ziplist或者hashtable。ziplist中的哈希对象是按照key1,value1,key2,value2这样的顺序存放来存储的。当对象数目不多且内容不大时，这种方式效率是很高的。 hashtable的是由dict这个结构来实现的, dict是一个字典，其中的指针dicht ht[2] 指向了两个哈希表 12345678910111213typedef struct dict { dictType *type; void *privdata; dictht ht[2]; long rehashidx; /* rehashing not in progress if rehashidx == -1 */ int iterators; /* number of iterators currently running */ } dict; typedef struct dictht { dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used; } dictht; dicht[0] 是用于真正存放数据，dicht[1]一般在哈希表元素过多进行rehash的时候用于中转数据。dictht中的table用语真正存放元素了，每个key/value对用一个dictEntry表示，放在dictEntry数组中。 集合对象 集合对象的编码可以是intset或者hashtableintset是一个整数集合，里面存的为某种同一类型的整数，支持如下三种长度的整数： 123#define INTSET_ENC_INT16 (sizeof(int16_t)) #define INTSET_ENC_INT32 (sizeof(int32_t)) #define INTSET_ENC_INT64 (sizeof(int64_t)) intset是一个有序集合，查找元素的复杂度为O(logN)，但插入时不一定为O(logN)，因为有可能涉及到升级操作。比如当集合里全是int16_t型的整数，这时要插入一个int32_t，那么为了维持集合中数据类型的一致，那么所有的数据都会被转换成int32_t类型，涉及到内存的重新分配，这时插入的复杂度就为O(N)了。intset不支持降级操作。 有序集合对象 有序集合的编码可能两种，一种是ziplist，另一种是skiplist与dict的结合。ziplist作为集合和作为哈希对象是一样的，member和score顺序存放。按照score从小到大顺序排列skiplist是一种跳跃表，它实现了有序集合中的快速查找，在大多数情况下它的速度都可以和平衡树差不多。但它的实现比较简单，可以作为平衡树的替代品。它的结构比较特殊。下面分别是跳跃表skiplist和它内部的节点skiplistNode的结构体： 123456789101112131415161718192021222324252627282930/* * 跳跃表 */ typedef struct zskiplist { // 头节点，尾节点 struct zskiplistNode *header, *tail; // 节点数量 unsigned long length; // 目前表内节点的最大层数 int level; } zskiplist; /* ZSETs use a specialized version of Skiplists */ /* * 跳跃表节点 */ typedef struct zskiplistNode { // member 对象 robj *obj; // 分值 double score; // 后退指针 struct zskiplistNode *backward; // 层 struct zskiplistLevel { // 前进指针 struct zskiplistNode *forward; // 这个层跨越的节点数量 unsigned int span; } level[]; } zskiplistNode; head和tail分别指向头节点和尾节点，然后每个skiplistNode里面的结构又是分层的(即level数组)用图表示，大概是下面这个样子： 总结以上简单介绍了Redis的简介，特性以及五种对象类型和五种对象类型的底层实现。事实上，Redis的高效性和灵活性正是得益于同一个对象类型采用不同的底层结构，并且在必要的时候对二者进行转换，还有就是各种底层结构对内存的合理利用。 本文作者：Worktile高级工程师 龚林杰 文章来源：Worktile技术博客","link":"/2019/11/26/redis-learning-2/"},{"title":"zookeeper学习之Paxos协议","text":"拜占庭将军问题 拜占庭位于如今的土耳其的伊斯坦布尔，是东罗马帝国的首都。由于当时拜占庭罗马帝国国土辽阔，为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军必需达成 一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，左右将军们的决定又扰乱整体军队的秩序，在进行共识时，结果并不代表大多数人的意见。这时候，在已知有成员不可靠的情况下，其余忠诚的将军在不受叛徒或间谍的影响下如何达成一致的协议，拜占庭问题就此形成。 ​ 拜占庭假设是对现实世界的模型化，由于硬件错误、网络拥塞或断开以及遭到恶意攻击，计算机和网络可能出现不可预料的行为。 ​ 在常见的分布式系统中，总会发生诸如机器宕机或网络异常（包括消息的延迟、丢失、重复、乱序，还有网络分区）等情况。 ​ Paxos算法需要解决的问题就是如何在一个可能发生上述异常的分布式系统中，快速且正确地在集群内部对某个数据的值达成一致，并且保证不论发生以上任何异常，都不会破坏整个系统的一致性。 历史／趣事： “Leslie Lamport也是用了长达9年的时间来完善这个算法的理论”– •Lamport大牛在他Paxos的第一个版本早在1990年就提交给ACM TOCS Jnl.的评审委员会了,但是当时没有人理解他的算法 ,主编回执他的稿子建议他用数学而不是神话描述他的算法 他们才会考虑接受这篇paper 。 •Lamport大牛很生气 他有次就在一个会议上说:”为什么搞理论的这群人一点幽默感也没有呢?”, 他拒绝修改 而且withdraw了这篇文章。 •1996年微软的Butler Lampson在WDAG96上提出了重新审视这篇文章 因为他读懂了 •1997年MIT的Nancy Lynch在WDAG97上 根据原文重新改写了这篇文章，Lamport用数学形式化的定义并证明了Paxos。 •于是在1998年的ACM TOCS上 这篇迟到了9年的paper终于被接受了。 •后来2001年 Lamport大牛也作出了让步 他用简单的语言而不是神话故事 重述了原文 ，但是通篇还是没有数学符号， L大牛甚为固执 ，据他自己说 ，他检查过了自己的语言并没有歧义 ，不需要数学来描述。 Google Chubby的作者说：“这个世界上只有一种一致性算法，那就是 （Paxos 时间线： [1]. LESLIE LAMPORT, ROBERT SHOSTAK, MARSHALL PEASE. The Byzantine General Problem. 1982 [2]. Leslie Lamport. The Part-Time Parliament. 1998 [3]. Leslie Lamport. Paxos Made Simple. 2001 [4]. Diego Ongaro and John Ousterhout. Raft Paper. 2013 [5]. Raft Website. The Raft Consensus Algorithm [6]. Raft Demo. Raft Animate Demo Paxos共识算法 ​ Paxos本来是虚构故事中的一个小岛，议会通过表决来达成共识。但是议员可能离开，信使可能走丢，或者重复传递消息。对应到分布式系统的节点故障和网络故障。 •假设议员要提议中午吃什么。如果有一个或者多个人同时提议，但一次只能通过一个提议，这就是Basic Paxos，是Paxos中最基础的协议。 •显然Basic Paxos是不够高效的，如果将Basic Paxos并行起来，同时提出多个提议，比如中午吃什么、吃完去哪里嗨皮、谁请客等提议，议员也可以同时通过多个提议。这就是Multi-Paxos协议。 Basic Paxos角色 Paxos算法存在3种角色：Proposer、Acceptor、Learner，在实现中一个节点可以担任多个角色。 •Proposer负责提出提案 •Acceptor负责对提案进行投票 •Learner获取投票结果，并帮忙传播 Learner不参与投票过程，为了简化描述，我们直接忽略掉这个角色。 算法 运行过程分为两个阶段 •Prepare阶段 •Accept阶段 •Proposer需要发出两次请求，Prepare请求和Accept请求。 •Acceptor根据其收集的信息，接受或者拒绝提案。 Prepare阶段 •Proposer选择一个提案编号n，发送Prepare(n)请求给超过半数（或更多）的Acceptor。 •Acceptor收到消息后，如果n比它之前见过的编号大，就回复这个消息，而且以后不会接受小于n的提案。另外，如果之前已经接受了小于n的提案，回复那个提案编号和内容给Proposer。 Accept阶段 •当Proposer收到超过半数的回复时，就可以发送Accept(n, value)请求了。 n就是自己的提案编号，value是Acceptor回复的最大提案编号对应的value，如果Acceptor没有回复任何提案，value就是Proposer自己的提案内容。 •Acceptor收到消息后，如果n大于等于之前见过的最大编号，就记录这个提案编号和内容，回复请求表示接受。 •当Proposer收到超过半数的回复时，说明自己的提案已经被接受。否则回到第一步重新发起提案。 共识过程情况1：提案已接受 •这个过程表示，S1收到客户端的提案X，于是S1作为Proposer，给S1-S3发送Prepare(3.1)请求，由于Acceptor S1-S3没有接受过任何提案，所以接受该提案。然后Proposer S1-S3发送Accept(3.1, X)请求，提案X成功被接受。 •在提案X被接受后，S5收到客户端的提案Y，S5给S3-S5发送Prepare(4.5)请求。对S3来说，4.5比3.1大，且已经接受了X，它会回复这个提案 (3.1, X)。S5收到S3-S5的回复后，使用X替换自己的Y，于是发送Accept(4.5, X)请求。S3-S5接受提案。最终所有Acceptor达成一致，都拥有相同的值X。 •这种情况的结果是：新**Proposer**会使用已接受的提案 情况2：提案未接受，新Proposer可见 •S3接受了提案(3.1, X)，但S1-S2还没有收到请求。此时S3-S5收到Prepare(4.5)，S3会回复已经接受的提案(3.1, X)，S5将提案值Y替换成X，发送Accept(4.5, X)给S3-S5，对S3来说，编号4.5大于3.1，所以会接受这个提案。 •然后S1-S2接受Accept(3.1, X)，最终所有Acceptor达成一致。 •这种情况的结果是：新Proposer会使用已提交的值，两个提案都能成功 情况3： 提案未接受，新Proposer不可见 •S1接受了提案(3.1, X)，S3先收到Prepare(4.5)，后收到Accept(3.1, X)，由于3.1小于4.5，会直接拒绝这个提案。所以提案X无法收到超过半数的回复，这个提案就被阻止了。提案Y可以顺利通过。 •这种情况的结果是：新Proposer使用自己的提案，旧提案被阻止 活锁 •活锁发生的几率很小，但是会严重影响性能。就是两个或者多个Proposer在Prepare阶段发生互相抢占的情形。 •解决方案是Proposer失败之后给一个随机的等待时间，这样就减少同时请求的可能。 Multi-Paxos •它会从Proposer中选出一个Leader，只由Leader提交Proposal，还可以省去Prepare阶段，减少了性能损失。当然，直接把Basic Paxos的多个Proposer的机制搬过来也是可以的，只是性能不够高。 •将Basic Paxos并行之后，就可以同时处理多个提案了，因此要能存储不同的提案，也要保证提案的顺序。 上文提到的活锁，也可以使用Multi-Paxos来解决 Multi-Paxos需要解决的问题 1. Leader选举 •一个最简单的选举方法，就是Server ID最大的当Leader。 •每个Server间隔T时间向其他Server发送心跳包，如果一个Server在2T时间内没有收到来自更高ID的心跳，那么它就成为Leader。 •其他Proposer，必须拒绝客户端的请求，或将请求转发给Leader。 •当然，还可以使用其他更复杂的选举方法，这里不再详述。 2.省略Prepare阶段 •Prepare的作用是阻止旧的提案，以及检查是否有已接受的提案值。 •当只有一个Leader发送提案的时候，Prepare是不会产生冲突的，可以省略Prepare阶段，这样就可以减少一半RPC请求。 •Prepare请求的逻辑修改为： –Acceptor记录一个全局的最大提案编号 –回复最大提案编号，如果当前entry以及之后的所有entry都没有接受任何提案，回复noMoreAccepted –当Leader收到超过半数的noMoreAccepted回复，之后就不需要Prepare阶段了，只需要发送Accept请求。直到Accept被拒绝，就重新需要Prepare阶段。 3. 完整信息流 •Basic Paxos只需超过半数的节点达成一致。但是在Multi-Paxos中，这种方式可能会使一些节点无法得到完整的entry信息。我们希望每个节点都拥有全部的信息。 •只有Proposer知道一个提案是否被接受了（根据收到的回复），而Acceptor无法得知此信息。 第1个问题的解决方案很简单，就是Proposer给全部节点发送Accept请求。 第2个问题稍微复杂一些。首先，我们可以增加一个Success RPC，让Proposer显式地告诉Acceptor，哪个提案已经被接受了，这个是完全可行的，只不过还可以优化一下，减少请求次数。 优化策略 在Accept请求中，增加一个firstUnchosenIndex参数，表示Proposer的第一个未接受的Index，这个参数隐含的意思是，对该Proposer来说，小于Index的提案都已经被接受了。因此Acceptor可以利用这个信息，把小于Index的提案标记为已接受。 另外要注意的是，只能标记该Proposer的提案，因为如果发生Leader切换，不同的Proposer拥有的信息可能不同，不区分Proposer直接标记的话可能会不一致。 Proposer正在准备提交Index=2的Accept请求，0和1是已接受的提案，因此firstUnchosenIndex=2。当Acceptor收到请求后，比较Index，就可以将Dumplings提案标记为已接受。 由于之前提到的Leader切换的情况，仍然需要显式请求才能获得完整信息。在Acceptor回复Accept消息时，带上自己的firstUnchosenIndex。如果比Proposer的小，那么就需要发送Success(index, value)，Acceptor将收到的index标记为已接受，再回复新的firstUnchosenIndex，如此往复直到两者的index相等。 这篇文章的分析来源于网络，具体没有记录来源，如有侵权，请联系我删除，谢谢！","link":"/2019/11/29/zk-paxos/"},{"title":"zookeeper学习前瞻","text":"ZK的由来12What is ZooKeeper? ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them, which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed. ​ Zookeeper 分布式服务框架是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。 ​ Zookeeper 最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。 ​ 所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。 ​ 关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的Pig项目)，雅虎的工程师希望给这个项目也取一个动物的名字。 ​ 时任研究院的首席科学家 Raghu Ramakrishnan 开玩笑地说：“在这样下去，我们这儿就变成动物园了！” ​ 此话一出，大家纷纷表示就叫动物园管理员吧，因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了。 ​ 而 Zookeeper 正好要用来进行分布式环境的协调，于是，Zookeeper 的名字也就由此诞生了。 ——《Paxos到 ZooKeeper 》 ZK的目标​ 将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。 原语： 操作系统或计算机网络用语范畴。它是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性，即原语的执行必须是连续的，在执行过程中不允许被中断。 文件系统 123名称空间由 ZooKeeper 中的数据寄存器组成，称为 Znode，这些类似于文件和目录。与为存储设计的典型文件系统不同，ZooKeeper 数据保存在内存中，这意味着 ZooKeeper 可以实现高吞吐量和低延迟。 有四种类型的znode： 1、PERSISTENT-持久化目录节点 客户端与zookeeper断开连接后，该节点依旧存在 2、 PERSISTENT_SEQUENTIAL-持久化顺序编号目录节点 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号 3、EPHEMERAL-临时目录节点 客户端与zookeeper断开连接后，该节点被删除 4、EPHEMERAL_SEQUENTIAL-临时顺序编号目录节点 客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号 通知机制 1Watcher （事件监听器） ： ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。 ZK的应用场景命名服务 在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现 配置管理 如果程序分散部署在多台机器上，要逐个改变配置就变得困难。好吧，现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好。 集群管理 1.机器退出和加入 所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知。新机器加入 也是类似，所有机器收到通知 2.选举master 举个最简单的例子：所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master 分布式锁 锁服务可以分为两类：一个是保持独占，另一个是控制时序。 独占锁：我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。用完删除掉自己创建的distribute_lock 节点就释放出锁。 时序锁：预先创建 /distribute_lock 节点，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除 队列管理 1.同步队列 ​ 当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 ​ 实现方案：在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目 2.FIFO 方式进行入队和出队操作 ​ 入列有编号，出列按编号 ZK可以实现的功能远远不止这些，这里只列出一些经典使用场景 ZK集群Master/Slave 模式（主备模式）: ​ 在这种模式中，通常 Master 服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。 角色 引入了Leader、Follower 和 Observer 三种角色。 1.Leader 既可以为客户端提供写服务又能提供读服务 2.Follower 和 Observer 都只能提供读服务 3.Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能 ZK一致性原理 ZK的一致性算法参照paxos算法实现，但不完全实现paxos算法。 ZAB 协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为 ZooKeeper 设计的崩溃可恢复的原子消息广播算法。 ZAB（ZooKeeper Atomic Broadcast 原子广播）协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播。 当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。 当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。 其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致。 当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进人消息广播模式了。 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播。 那么新加入的服务器就会自觉地进人数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 请看下文paxos解读","link":"/2019/11/29/zk-pre/"}],"tags":[{"name":"DUBBO","slug":"DUBBO","link":"/tags/DUBBO/"},{"name":"MOCK","slug":"MOCK","link":"/tags/MOCK/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"LINUX","slug":"LINUX","link":"/tags/LINUX/"},{"name":"I/O","slug":"I-O","link":"/tags/I-O/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"J2EE","slug":"J2EE","link":"/tags/J2EE/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"JAVA","slug":"JAVA","link":"/tags/JAVA/"},{"name":"THREAD","slug":"THREAD","link":"/tags/THREAD/"},{"name":"JDK","slug":"JDK","link":"/tags/JDK/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"linux ","slug":"linux","link":"/tags/linux/"},{"name":"MQ","slug":"MQ","link":"/tags/MQ/"},{"name":"MYSQL","slug":"MYSQL","link":"/tags/MYSQL/"},{"name":"NETTY","slug":"NETTY","link":"/tags/NETTY/"},{"name":"REDIS","slug":"REDIS","link":"/tags/REDIS/"},{"name":"SPRING","slug":"SPRING","link":"/tags/SPRING/"},{"name":"ZK","slug":"ZK","link":"/tags/ZK/"},{"name":"eureka","slug":"eureka","link":"/tags/eureka/"},{"name":"LOG","slug":"LOG","link":"/tags/LOG/"}],"categories":[]}